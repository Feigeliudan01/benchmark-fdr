---
title: "Manuscript Figures for Genome Biology"
author: "Rafalab"
output: 
    html_document:
        toc: true
        toc_float: true
        highlight: tango
        number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Figure 1

This is a schema of Inputs, Assumptions and Outputs for the various FDR methods. Claire is leading this charge. 

# Figure 2

This is a faceted figure with results using the yeast data with 48 biological replicates analyzed in this [publication](https://www.ncbi.nlm.nih.gov/pubmed/26206307/). A complete analysis 
can be found in the `/datasets/RNA-Seq/yeast-simulation.Rmd`. This is the 5v5 comparison.  

```{r Figure2-load-data}
# Load packages and source benchmark FDR
library(data.table)
library(tidyr)
library(dplyr)
library(readr)
library(ggplot2)
library(magrittr)
library(cowplot)
library(purrr)
library(DESeq2)
library(tibble)
library(ggthemes)
library(R.utils)

## load helper functions
for (f in list.files("../R", "\\.(r|R)$", full.names = TRUE)) {
    source(f)
}

# set up data / results directories
resdir <- "yeast-results"
outdir <- "yeast-figs"
dir.create(outdir, showWarnings = FALSE)
```


```{r Figure2}
excludeSet <- c("unadjusted", "bl-df02", "bl-df04", "bl-df05")
resfile <- file.path(resdir, "de5.rds")
de5 <- readRDS(file=resfile)
res5d <- plotsim_standardize(de5, alpha = seq(0.01, 0.10, 0.01))

# Figure 2A: Not all methods control FDR at nominal alpha level
p1 <- plotsim_average(res5d, met="FDR",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) +
      theme(legend.position="none")

# Figure 2B: TPR of methods across nominal alpha level
p2 <- plotsim_average(res5d, met="TPR",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) +
      theme(legend.position="none")

# Figure 2C: Total number of discoveries varied across method
p3 <- plotsim_average(res5d, met="rejections",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) +
      theme(legend.position="none")

# Figure 2D: Some methods are tailored to detecting smaller effect sizes
p4 <- covariateLinePlot(de5, alpha=0.05, covname="log2FC", trans="log1p",
                        nbins=25)
leg <- get_legend(p4)
p4 <- p4 + theme(legend.position="none")

# maybe 2E? - doesn't return a ggplot object :( 
# fails to convert using ggplotify package
aggupset(de5, alpha=0.05, supplementary = FALSE, 
                       return_list = FALSE)

# Figure 2 (ALL)
Fig2 <- plot_grid(p1, p2, p3, p4, ncol = 2, 
          labels = LETTERS[1:4], label_size = 20)
Fig2 <- plot_grid(Fig2, leg, ncol = 2, rel_widths = c(1, 0.2))
#Fig2 <- plot_grid(Fig2, p5, ncol=1, rel_heights = c(1, 0.5),
#                 labels = c("", "E"), label_size = 20)
Fig2
ggsave(file.path(outdir, "Figure2.pdf"), width=10, height=8)
```

# Figure 2

This is a side-by-side figure displaying the TPR by alpha cutoff, faceted by 
whether the method uses multiple pieces of information or just one (the p-value).
This figure uses the same data as Figure 1.
  
```{r, Figure3}
# Figure 3: TPR of methods using by pieces of information used 
Fig3 <- plotsim_average(res5d, met="TPR", filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE, facetMethodType = TRUE) 

Fig3
ggsave(file.path(outdir, "Figure3.pdf"), width=8, height=4)
```

# Figure 4 

This is a heatmap of methods by case studies with color representing the ranking
by of number of rejections @ FDR cutoff 0.10.

```{r, Figure 4}
library(SummarizedBenchmark)
library(stringr)
library(viridis)

## load helper functions
for (f in list.files("../R", "\\.(r|R)$", full.names = TRUE)) {
    source(f)
}

outdir <- "casestudy-figs"
dir.create(outdir, showWarnings = FALSE)

# assumes sb objects are in the following location, which contains subfolders
# for each casestudy (if this isn't true, then parsing the case study and 
# dataset names later on will be incorrect)
path <- "../../sb-objects/"
objects <- list.files( path, recursive=TRUE, pattern="rds", full.names=TRUE )
alpha <- 0.10

# remove simulation objects
sim <- unique(c(which(grepl("simulations", objects)),
               which(grepl("yeast", objects)),
               which(grepl("polyester", objects))))

if (length(sim) > 0){
  objects <- objects[-sim]
}

# create tidy data frame where each row is a method / dataset observation
# of a rank 
ranks <- data.frame()
obj.names <- str_split(objects, pattern = "/")
casestudy <- unlist(lapply(obj.names, function(x) x[5] ))
dataset <- unlist(lapply(obj.names, function(x) x[6] ))

for (i in seq_along(objects)){
  x <- readRDS(objects[i])
  assayNames(x) <- "qvalue"
  x <- addDefaultMetrics( x )

  hasResults <- apply(!is.na( assays(x)[["qvalue"]] ), 2, all )

  tmp <- estimatePerformanceMetrics(x, alpha, tidy=TRUE) %>%
    filter( performanceMetric == "rejections") %>%
    rename( method = blabel) %>%
    filter( is.na(param.alpha) | (param.alpha == alpha)) %>%
    filter( is.na(param.smooth.df) | (param.smooth.df == "3L")) %>%
    filter( !method == "unadjusted") %>%
    select( method, value ) %>%
    rename( nrejects = value) %>%
    mutate( method = gsub("-df03", "", method)) %>%
    mutate( method = gsub("-a10", "", method)) %>%
    na.omit() %>%
    mutate( rank = rank(nrejects) / n() ) %>%
    mutate(casestudy = casestudy[i],
           dataset = dataset[i])
  
  ranks <- rbind(ranks, tmp)
}

# average over datasets within case study
ranks_avg <- ranks %>% 
  group_by( casestudy, method) %>%
  summarize( mean_rank = mean(rank, na.rm = TRUE))

# heatmap : rows method, cols casestudy
Fig4 <- ggplot(ranks_avg, aes(x = casestudy, y = method, fill = mean_rank)) + 
  geom_raster() + 
  scale_fill_viridis() +
  theme_bw() +
  xlab("Case Study") +
  ylab("Method") +
  labs(fill = "Mean rank")

Fig4
ggsave(file.path(outdir, "Figure4.pdf"), width=6, height=4, dev = pdf)
```

# Figure 5

This is a multi-panel Figure displaying examples of covariate line plots in the
case studies.

```{r, Figure 5}
# TO DO
```

# Figure 6

This is a multi-panel Figure displaying examples of covariate vs p-value scatterplots
and/or histograms in the case studies. 
It will be used to discuss how we evaluated the use of the 
covariates as independent and informative in the case studies.

```{r, Figure 6}
# TO DO
```


# Session information

```{r}
sessionInfo()
```
