---
title: "Yeast RNA-seq 48 sample simulation study"
author: "Keegan Korthauer"
output: 
    html_document:
        toc: true
        toc_float: true
        highlight: tango
        number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This is an analysis of the yeast data with 48 biological replicates in each of 
two conditions (analyzed in this [publication](https://www.ncbi.nlm.nih.gov/pubmed/26206307/)). 
We chose this experiment because of the large number of biological replicates, which
will allow us to (1) implement null comparisons on random subsets of samples within
one condition, and (2) start with a null comparison and add in artificial differences
to a subset of genes to define 'true positives'. 

We will investigate the overall mean expression as an informative and independent 
(under the null hypothesis) covariate. 
We will also investigate the effects of an uninformative covariate by using a 
randomly generated covariate.

Processed count table is made available by the authors in their paper codebase 
[GitHub repository](https://github.com/bartongroup/profDGE48). 

In this Rmd we will carry out simulations for multiple
replicates, and plot results averaging over the replications. Here we draw effect
sizes (log2 fold changes) from the observed fold changes in the full comparison of the 
two conditions. Since this results in a distribution of effect sizes and test
statistics under the alternative that has a mode at zero, the assumptions for 
FDRreg-empirical are violated. For this reason, we do not include this method in
the benchmark comparisons.

# Set up workspace

```{r, results='hide', message=FALSE}
# Load packages and source benchmark FDR
library(SummarizedBenchmark)
library(data.table)
library(tidyr)
library(dplyr)
library(readr)
library(ggplot2)
library(magrittr)
library(cowplot)
library(purrr)
library(DESeq2)
library(tibble)
library(ggthemes)
library(R.utils)

## load helper functions
for (f in list.files("../R", "\\.(r|R)$", full.names = TRUE)) {
    source(f)
}

# set up data / results directories
datdir <- "yeast-data"
resdir <- "../../results/RNAseq"
dir.create(datdir, showWarnings = FALSE)
dir.create(resdir, showWarnings = FALSE)

# results files that will be generated
resfile_n5 <- file.path(resdir, "yeast-results-null5.rds")
resfile_n10 <- file.path(resdir, "yeast-results-null10.rds")
resfile_d5 <- file.path(resdir, "yeast-results-de5.rds")
resfile_d10 <- file.path(resdir, "yeast-results-de10.rds")

resfile_n5_uninfCov <- file.path(resdir, "yeast-results-null5-uninfCov.rds")
resfile_n10_uninfCov <- file.path(resdir, "yeast-results-null10-uninfCov.rds")
resfile_d5_uninfCov <- file.path(resdir, "yeast-results-de5-uninfCov.rds")
resfile_d10_uninfCov <- file.path(resdir, "yeast-results-de10-uninfCov.rds")

# set up parallel backend
library(parallel)
nCores <- 20
```

# Data download

First, we download the processed count data from GitHub. There is one file for the 
Snf2 condition and one file for the wild type condition. The Snf2 condition is a yeast
strain that has the transcriptional regulator gene Snf2 knocked out. Each of these
files is a compressed `tar.gz` archive that contains a 
single bam file for each replicate in each condition, which we'll place in a subdirectory
called `r datdir`.

```{r, download}
download.file(url = "https://github.com/bartongroup/profDGE48/raw/master/Preprocessed_data/Snf2_countdata.tar.gz",
              destfile = file.path(datdir, "Snf2_countdata.tar.gz"))
download.file(url = "https://github.com/bartongroup/profDGE48/raw/master/Preprocessed_data/WT_countdata.tar.gz", 
              destfile = file.path(datdir, "WT_countdata.tar.gz"))

gunzip(file.path(datdir, "Snf2_countdata.tar.gz")) 
gunzip(file.path(datdir, "WT_countdata.tar.gz")) 

untar(file.path(datdir, "Snf2_countdata.tar"), exdir = datdir) 
untar(file.path(datdir, "WT_countdata.tar"), exdir = datdir) 

file.remove(file.path(datdir, "Snf2_countdata.tar"), 
            file.path(datdir, "WT_countdata.tar"))
```

Each of the data files contains two columns, one with a gene/feature name, and one
with the count value.

We'll also download the list of 'bad' replicates which were especially poorly 
correlated to the others, as determined by the authors.

```{r}
download.file(url = "https://github.com/bartongroup/profDGE48/raw/master/Bad_replicate_identification/exclude.lst", destfile = (file.path(datdir, "badreps.txt")))
```

## Read data into R and create a count table

Here we make use of the map and map2 functions in the `purrr` package, to swiftly 
apply the `read_tsv` function from `readr` to read in all of the 96 sample tables, 
as well as add in the sample name (derived from the file name) to each subtable.
Finally, the `reduce` function is used to join all the replicates together.
We will remove samples that failed QC in the original study.

```{r, readin, results='hide', message=FALSE}
files <- dir(path = datdir, pattern = "*.bam.gbgout", full.names = TRUE)
sample_names <- sapply(strsplit(dir(path = datdir, pattern = "*.bam.gbgout"), "_MID"),
                       function(x) x[[1]])
badreps <- read_tsv(file.path(datdir, "badreps.txt"), col_names = FALSE)$X1
badreps <- unlist(lapply(strsplit(badreps, "_MID"), function(x) x[1]))

counts <- files %>%
  purrr::map(read_tsv, col_names = FALSE) %>% # read in all the files individually
  purrr::map2(sample_names, ~ dplyr:::rename(.x, !! .y := X2, feature = X1) ) %>% # add sample names
  purrr::reduce(left_join, by = "feature") %>% # reduce with rbind into one dataframe
  dplyr::select(-badreps ) # remove badreps
```

# Analysis of full 48v48 (non-null)

Here we'll carry out an analysis of the full dataset, comparing the controls to the
knockout samples. The fold changes observed in this comparison will be used when 
generating the non-null simulated data in the following sections.

We're ready to construct a DESeq2 object. First we pull out the feature names and
add them as rownames for the count table, and next we construct a column data object
that houses the sample names, replicate numbers, and condition factor (WT versus
Snf2 knockout). 

## Set up DESeq2 object 

First we set up the DESeq2 object.

```{r, deseqsetup}
feats <- (counts %>% select(1))$feature
counts <- as.matrix(counts %>% select(-1))
rownames(counts) <- feats

coldat <- tibble(sample=colnames(counts)) %>% 
  separate(sample, sep="_", into=c("condition", "replicate"), remove=FALSE) %>%
  mutate(condition = factor(condition))

# filter low count genes
counts <- counts[rowMeans(counts) > 1,]

dds <- DESeqDataSetFromMatrix(countData = counts,
                              colData = coldat,
                              design= ~ condition)
```

## Run DESeq2

Next we run DESeq2.

```{r}
# results on full set
dds <- DESeq(dds)
resultsNames(dds) # lists the coefficients
resFULL <- results(dds, name="condition_WT_vs_Snf2", independentFiltering = F)

head(resFULL)

sum(resFULL$padj < 0.05, na.rm=T)
```

Check densities of the test statistic and effect sizes.

```{r}
data.frame(resFULL) %>% 
  ggplot(aes(x = stat)) +
  geom_density() +
  xlab("Test Statistic") 

data.frame(resFULL) %>% 
  ggplot(aes(x = log2FoldChange)) +
  geom_density() +
  xlab("Effect Size (log2 Fold Change") 
```

## Check assumptions

There are several thousand genes detected using the full set. Next, we'll build input data frame for summarized benchmark.

```{r}
geneExp <- tbl_df(data.frame(geneName=rownames(resFULL), 
                             pval=resFULL$pvalue, 
                             SE=resFULL$lfcSE,
                             ind_covariate = resFULL$baseMean, 
                             effect_size=resFULL$log2FoldChange, 
                             test_statistic=resFULL$stat,
                             pzero=rowSums(counts(dds)==0)/ncol(counts(dds))))

# filter NAs and those with less than 50% expressed 
geneExp <- geneExp %>% na.omit() %>% dplyr::filter(pzero < 0.5)
```

We'll create a plot to examine the distribution of effect sizes, since
the ash method assumes that the distribution of true (unobserved) effect
sizes is unimodal.

```{r}
ggplot(data=geneExp, aes(effect_size)) +
  geom_histogram(bins=30)
```

We'll also explore how the standard error (used by ash) 
correlates with the independent covariate (used by methods that incorporate 
covariates), in order to get an idea of how these pieces of information relate
to one another.

```{r}
ggplot(data=geneExp, aes(x = ind_covariate, y = SE)) +
  geom_hex(bins = 100) +
  scale_x_continuous(trans="log10") +
  xlab("Covariate: Mean gene expression") 
```


Look at covariate diagnostic plots.

```{r, width=15, height=15}
strat_hist(geneExp, pvalue="pval", covariate="ind_covariate", maxy=32)
```

```{r}
rank_scatter(geneExp, pvalue="pval", covariate="ind_covariate")
```

## FDR benchmarking

Build common bench design. We also add in Scott's FDR Regression (only
`nulltype = "theoretical"` since assumptions of the empirical method are violated)
since our test statistics are approximately t-distributed. 

```{r}
bd <- initializeBenchDesign()
bd <- addBMethod(bd, "fdrreg-t",
                     FDRreg::FDRreg,
                     function(x) { x$FDR },
                     z = test_statistic,
                     features = model.matrix( ~  splines::bs(ind_covariate, df = 3) - 1),
                     nulltype = 'theoretical',
                     control = list(lambda = 0.01))
```

Run benchmark methods. 
```{r}
sb <- bd %>% buildBench(data=geneExp, parallel = FALSE)

assayNames(sb) <- "qvalue"
sb <- addDefaultMetrics(sb)
```

Plot results.

```{r}
rejections_scatter(sb, supplementary=FALSE)
```

```{r}
plotFDRMethodsOverlap(sb, alpha=0.05, nsets=ncol(sb), order.by="freq", decreasing=TRUE, supplementary=FALSE)

```

# Simulation set up

Next, we'll analyze random splits of one condition, both with and without the 
addition of simulated DE genes. Here we'll create a function that we can use to 
run one replicate given sample size and number of DE gene settings. This will be
looped over many replications and results averaged over them.

```{r}
#' @param X simnumber
#' @param seed random seed
#' @param sampleSize is the number of samples in each condition
#' @param nDE is the number of DE genes
#' @param bd is the bench design object
#' @param BPPARAM is the BiocParallel bpparam argument to pass to DESeq. Since
#'  parallelization happens by forking multiple instances of this function, we 
#'  will specify only one worker per DEseq intance.
#' @param uninformativeCovariate logical indicating whether to replace overall mean 
#'  independent covariate with a randomly generated (uninformative) covariate. 
#'  Default is FALSE.
#' @param pvalHists logical whether to return histograms of pvalues by covariate
#'  instead of SB objects
simulateOneSplit <- function(X, rseed, nDE, sampleSize,
                             bd, uninformativeCovariate = FALSE, pvalHists = FALSE){
  
  uninf_covar <- rnorm(nrow(dds))
  
  # set random seed
  set.seed(as.numeric(X)*as.numeric(rseed))
  
  # select a random subset of 20 WT samples
  dds_test <- dds[,sample(1:ncol(dds), sampleSize*2)]

  # add a fake condition column to coldat
  colData(dds_test)$fake <-  factor(c(rep("A", sampleSize), 
                                      rep("B", sampleSize))[sample(1:(sampleSize*2), 
                                                                   sampleSize*2)])
  design(dds_test) <- ~fake
  
  pzero = rowSums(counts(dds_test)==0)/ncol(counts(dds_test))
  dds_test <- dds_test[pzero < 0.5,]
  uninf_covar <- uninf_covar[pzero < 0.5]
  
  truth <- rep(FALSE, nrow(dds_test))
  
  # make sure null comparison is truly null: if PC1 or PC2 sig different, 
  # reshuffle sample labels
  dds_test <- estimateSizeFactors(dds_test)
  x <- t(counts(dds_test, normalize=TRUE))
  pc <- prcomp(log(x + 0.5), scale.=TRUE)

  a1 <- pc$x[colData(dds_test)$fake=="A",1]
  b1 <- pc$x[colData(dds_test)$fake=="B",1] 
  a2 <- pc$x[colData(dds_test)$fake=="A",2]
  b2 <- pc$x[colData(dds_test)$fake=="B",2] 
  p1 <- wilcox.test(a1, b1)$p.value
  p2 <- wilcox.test(a2, b2)$p.value
  tries <- 0
  
  while(p1 < 0.10 || p2 < 0.10 && tries < 5){
    colData(dds_test)$fake <- sample(colData(dds_test)$fake, ncol(dds_test))
    x <- t(counts(dds_test, normalize=TRUE))
    pc <- prcomp(log(x + 0.5), scale.=TRUE)

    a1 <- pc$x[colData(dds_test)$fake=="A",1]
    b1 <- pc$x[colData(dds_test)$fake=="B",1] 
    a2 <- pc$x[colData(dds_test)$fake=="A",2]
    b2 <- pc$x[colData(dds_test)$fake=="B",2] 
    p1 <- t.test(a1, b1)$p.value
    p2 <- t.test(a2, b2)$p.value
    
    tries <- tries + 1
  }
  
  if(nDE > 0){
    # pick random set of nDE genes to add signal to
    wts <- rowMeans(counts(dds_test, normalize = TRUE))
    DE <- sample(1:nrow(dds_test), nDE, prob = wts)
    truth[DE] <- TRUE
    
    # randomly sample a log2FC from original FCs (without regard to DE)
    counts_new <- counts(dds_test)
    log2FC <- rep(0, nrow(dds_test))
    log2FC[DE] <- resFULL$log2FoldChange[pzero < 0.5][DE]
    
    
    # randomize which condition is shifted up or down
    ran <- runif(nrow(dds_test)) 
    refcond <- ifelse(ran < 0.5, "A", "B")
    down <- which(ran < 0.5)
    
    counts_new[down,colData(dds_test)$fake==unique(refcond[down])] <- 
        counts(dds_test)[down, colData(dds_test)$fake==unique(refcond[down])] *
        2^log2FC
    counts_new[-down,colData(dds_test)$fake==unique(refcond[-down])] <- 
        counts(dds_test)[-down, colData(dds_test)$fake==unique(refcond[-down])] *
        2^log2FC
           
    counts_new <- apply(counts_new, 2, as.integer)
    
    
    counts(dds_test) <- counts_new
  }
  
  # replace existing size factors 
  dds_test <- estimateSizeFactors(dds_test)
  
  dds_test <- DESeq(dds_test, parallel = FALSE)
  resTEST <- results(dds_test, name="fake_B_vs_A", independentFiltering = FALSE)
  
  geneExp <- tbl_df(data.frame(geneName=rownames(resTEST), 
                               pval=resTEST$pvalue, 
                               SE=resTEST$lfcSE,                 
                               ind_covariate = resTEST$baseMean,
                               effect_size = resTEST$log2FoldChange, 
                               test_statistic = resTEST$stat,
                               qvalue = truth))
  
  if (uninformativeCovariate)
    geneExp <- mutate(geneExp, ind_covariate = uninf_covar)
  
  geneExp <-  geneExp %>% dplyr::filter(!is.na(pval))
  
  if(pvalHists){
    return(strat_hist(geneExp, pvalue="pval", covariate="ind_covariate", maxy =10))
  }

  sb <- bd %>% buildBench(data=geneExp, parallel = FALSE, 
                          truthCols = "qvalue",
                          ftCols = "ind_covariate")
  
  assayNames(sb) <- "qvalue"
  sb <- addDefaultMetrics(sb)
  rowData(sb)$log2FC <- geneExp$effect_size
    
  return(sb)
}
 
```



We'll also set some parameters that will be common to all simulations. These
include the number of replications, the bench design object, the set of 
methods to exclude in the results plots, and the alpha cutoff level to 
be used when plotting the aggregated Upset results.

```{r}
B <- 100
excludeSet <- c("unadjusted", "bl-df02", "bl-df04", "bl-df05")
ualpha <- 0.05

# only keep one condition for subsetting in simulations that follow 
dds <- dds[,colData(dds)$condition == "Snf2"]
```

Here's a helper function to return the number of methods with rejections at
a particular alpha level (this helps us determine whether or not to plot the
aggregated upset plot - if there aren't at least 2 methods it will throw an
error, which is a problem for the null simulations).

```{r}
# To be included in the upset agg plot, method needs to have found on average
# at least one rejection per replicate. To create an upset plot, require that
# at least two methods rejected at this threshold.
#' @param res standardized metric data.table generated using
#'        standardize_results.
#' @param alpha alpha cutoff
#' @param filterSet which methods to exclude from consideration 
numberMethodsReject <- function(res, alphacutoff, filterSet){
  res <- res %>% 
    filter(is.na(param.alpha) | (param.alpha == alphacutoff)) %>%
    filter(!(blabel %in% filterSet)) %>%
    filter(alpha == alphacutoff) %>%
    filter(performanceMetric == "rejections") %>%
    select(blabel, performanceMetric, value) %>%
    group_by(blabel) %>%
    summarize(mean_value = mean(value)) %>%
    filter(mean_value > 1)
  return(nrow(res))
}

```


# Sim N5: Null Comparison 5v5

Here we'll repeat the above, but for a null comparison 5 versus 5 Snf2 samples,
where the groups are selected randomly. This will be done for 100 random splits.

## Generate a list of SB results objects

```{r n5, results='hide', message=FALSE}
rseed <- 225
sampleSize <- 5
nDE <- 0

if (!file.exists(resfile_n5)){
  null5 <- mclapply(X=1:B, FUN=simulateOneSplit, rseed=rseed, 
                    nDE=nDE, sampleSize=sampleSize, bd=bd, mc.cores=nCores)
  saveRDS(null5, file = resfile_n5)
}else{
  null5 <- readRDS(file = resfile_n5)
}
```

## Plot average results over replications

Plot results. 

```{r}
# Check for missing results (if any methods threw an error for relevant metrics).
rowSums(sapply(null5, function(x) colSums(is.na(assays(x)$qvalue)) > 0))

res5 <- plotsim_standardize(null5, alpha = seq(0.01, 0.10, 0.01))

plotsim_average(res5, met="rejections",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 
plotsim_average(res5, met="TNR",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 

covariateLinePlot(null5, alpha=0.05, covname="log2FC", nbins=25, 
                 trans="log1p")
covariateLinePlot(null5, alpha=0.05, covname="ind_covariate", nbins=25, 
                 trans="log1p")

if (numberMethodsReject(res5, alphacutoff=ualpha, filterSet=excludeSet) >= 2){
  aggupset(null5, alpha=ualpha, supplementary = FALSE, return_list = FALSE) 
}else{
  message("Not enough methods found rejections at alpha ", ualpha, 
          "; skipping upset plot")
}

```

# Sim N10: Null Comparison 10v10

Here we'll repeat the above, but for a null comparison 10 versus 10 Snf2 samples,
where the groups are selected randomly. This will be done for 100 random splits.

## Generate a list of SB results objects

```{r n10, results='hide', message=FALSE}
rseed <- 837
sampleSize <- 10
nDE <- 0

if (!file.exists(resfile_n10)){
  null10 <- mclapply(X=1:B, FUN=simulateOneSplit, rseed=rseed, 
                    nDE=nDE, sampleSize=sampleSize, bd=bd, mc.cores=nCores)
  saveRDS(null10, file = resfile_n10)
}else{
  null10 <- readRDS(file = resfile_n10)
}

```

## Plot average results over replications

Plot results.

```{r}
# Check for missing results (if any methods threw an error for relevant metrics).
rowSums(sapply(null10, function(x) colSums(is.na(assays(x)$qvalue)) > 0))

res10 <- plotsim_standardize(null10, alpha = seq(0.01, 0.10, 0.01))

plotsim_average(res10, met="rejections",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 
plotsim_average(res10, met="TNR",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 

covariateLinePlot(null10, alpha=0.05, covname="log2FC", nbins=25, 
                 trans="log1p")
covariateLinePlot(null10, alpha=0.05, covname="ind_covariate", nbins=25, 
                 trans="log1p")

if (numberMethodsReject(res10, alphacutoff=ualpha, filterSet=excludeSet) >= 2){
  aggupset(null10, alpha=ualpha, supplementary = FALSE, return_list = FALSE) 
}else{
  message("Not enough methods found rejections at alpha ", ualpha, 
          "; skipping upset plot")
}

```



# Sim D5: DE Comparison 5v5

Here we'll repeat the above, but for a DE comparison 5 versus 5 Snf2 samples,
where the groups are selected randomly and 2000 DE genes are added. 
This will be done for 100 random splits.

## Generate a list of SB results objects

```{r d5, results='hide', message=FALSE}
rseed <- 198
sampleSize <- 5
nDE <- 2000

if (!file.exists(resfile_d5)){
  de5 <- mclapply(X=1:B, FUN=simulateOneSplit, rseed=rseed, 
                  nDE=nDE, sampleSize=sampleSize, bd=bd, mc.cores=nCores)
  saveRDS(de5, file = resfile_d5)
}else{
  de5 <- readRDS(file = resfile_d5)
}

# pvalue histograms
plotfile <- file.path(datdir, "de5_pvalhists.pdf")
if (!file.exists(plotfile)){
  hists <- mclapply(X=1:B, FUN=simulateOneSplit, rseed=rseed, 
                   nDE=nDE, sampleSize=sampleSize, bd=bd, 
                   pvalHists = TRUE, mc.cores = nCores)
  pdf(plotfile, width=8, height=4)
  for(i in 1:length(hists)){
    print(hists[[i]])
  }
  dev.off()
}
```

## Plot average results over replications

Plot results. 

```{r}
# Check for missing results (if any methods threw an error for relevant metrics).
rowSums(sapply(de5, function(x) colSums(is.na(assays(x)$qvalue)) > 0))

res5d <- plotsim_standardize(de5, alpha = seq(0.01, 0.10, 0.01))

plotsim_average(res5d, met="rejections",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 
plotsim_average(res5d, met="FDR",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 
plotsim_average(res5d, met="TPR",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 

covariateLinePlot(de5, alpha=0.05, covname="log2FC", nbins=25, 
                 trans="log1p")
covariateLinePlot(de5, alpha=0.05, covname="ind_covariate", nbins=25, 
                 trans="log1p")

if (numberMethodsReject(res5d, alphacutoff=ualpha, filterSet=excludeSet) >= 2){
  aggupset(de5, alpha=ualpha, supplementary = FALSE, return_list = FALSE) 
  
}else{
  message("Not enough methods found rejections at alpha ", ualpha, 
          "; skipping upset plot")
}

```

# Sim D10: DE Comparison 10v10

Here we'll repeat the above, but for a DE comparison 10 versus 10 Snf2 samples,
where the groups are selected randomly and 2000 DE genes are added.
This will be done for 100 random splits.

## Generate a list of SB results objects

```{r d10, results='hide', message=FALSE}
rseed <- 961
sampleSize <- 10
nDE <- 2000

if(!file.exists(resfile_d10)){
  de10 <- mclapply(X=1:B, FUN=simulateOneSplit, rseed=rseed, 
                  nDE=nDE, sampleSize=sampleSize, bd=bd, mc.cores=nCores)
  saveRDS(de10, file = resfile_d10)
}else{
  de10 <- readRDS(file = resfile_d10)
}

# pvalue histograms
plotfile <- file.path(datdir, "de10_pvalhists.pdf")
if (!file.exists(plotfile)){
  hists <- mclapply(X=1:B, FUN=simulateOneSplit, rseed=rseed, 
                   nDE=nDE, sampleSize=sampleSize, bd=bd, 
                   pvalHists = TRUE, mc.cores = nCores)
  pdf(plotfile, width=8, height=4)
  for(i in 1:length(hists)){
    print(hists[[i]])
  }
  dev.off()
}
```

## Plot average results over replications

Plot results.

```{r}
# Check for missing results (if any methods threw an error for relevant metrics).
rowSums(sapply(de10, function(x) colSums(is.na(assays(x)$qvalue)) > 0))

res10d <- plotsim_standardize(de10, alpha = seq(0.01, 0.10, 0.01))

plotsim_average(res10d, met="rejections",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 
plotsim_average(res10d, met="FDR",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 
plotsim_average(res10d, met="TPR",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 

covariateLinePlot(de10, alpha=0.05, covname="log2FC", nbins=25, 
                 trans="log1p")
covariateLinePlot(de10, alpha=0.05, covname="ind_covariate", nbins=25, 
                 trans="log1p")

if (numberMethodsReject(res10d, alphacutoff=ualpha, filterSet=excludeSet) >= 2){
  aggupset(de10, alpha=ualpha, supplementary = FALSE, return_list = FALSE) 
}else{
  message("Not enough methods found rejections at alpha ", ualpha, 
          "; skipping upset plot")
}

```

# Sample size comparison

Here we compare the method ranks for the different sample sizes at alpha = 0.10.

```{r}
plotMethodRanks(c(resfile_d5, resfile_d10), 
                colLabels = c("DE 5", "DE 10"), 
                alpha = 0.10, xlab = "Comparison", 
                excludeMethods = NULL)
```


# Uninformative covariate

Here we repeat the previous four sections, using an uninformative covariate 
instead of the overall mean expression.

## Sim N5: Null Comparison 5v5

Here we'll repeat the above, but for a null comparison 5 versus 5 Snf2 samples,
where the groups are selected randomly. This will be done for 100 random splits.

### Generate a list of SB results objects

```{r n5-uc, results='hide', message=FALSE}
rseed <- 225
sampleSize <- 5
nDE <- 0

if (!file.exists(resfile_n5_uninfCov)){
  null5 <- mclapply(X=1:B, FUN=simulateOneSplit, rseed=rseed,
                   nDE=nDE, sampleSize=sampleSize, bd=bd,
                   uninformativeCovariate = TRUE, mc.cores = nCores)
  saveRDS(null5, file = resfile_n5_uninfCov)
}else{
  null5 <- readRDS(file = resfile_n5_uninfCov)
}
```

### Plot average results over replications

Plot results. 

```{r}
# Check for missing results (if any methods threw an error for relevant metrics).
rowSums(sapply(null5, function(x) colSums(is.na(assays(x)$qvalue)) > 0))

res5 <- plotsim_standardize(null5, alpha = seq(0.01, 0.10, 0.01))

plotsim_average(res5, met="rejections",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 
plotsim_average(res5, met="TNR",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 

covariateLinePlot(null5, alpha=0.05, covname="log2FC", nbins=25, 
                 trans="log1p")
covariateLinePlot(null5, alpha=0.05, covname="ind_covariate", nbins=25, 
                 trans="log1p")

if (numberMethodsReject(res5, alphacutoff=ualpha, filterSet=excludeSet) >= 2){
  aggupset(null5, alpha=ualpha, supplementary = FALSE, return_list = FALSE) 
}else{
  message("Not enough methods found rejections at alpha ", ualpha, 
          "; skipping upset plot")
}

```

## Sim N10: Null Comparison 10v10

Here we'll repeat the above, but for a null comparison 10 versus 10 Snf2 samples,
where the groups are selected randomly. This will be done for 100 random splits.

### Generate a list of SB results objects

```{r n10-uc, results='hide', message=FALSE}
rseed <- 837
sampleSize <- 10
nDE <- 0

if (!file.exists(resfile_n10_uninfCov)){
  null10 <- mclapply(X=1:B, FUN=simulateOneSplit, rseed=rseed, 
                   nDE=nDE, sampleSize=sampleSize, bd=bd,
                   uninformativeCovariate = TRUE, mc.cores=nCores)
  saveRDS(null10, file = resfile_n10_uninfCov)
}else{
  null10 <- readRDS(file = resfile_n10_uninfCov)
}
```

### Plot average results over replications

Plot results.

```{r}
# Check for missing results (if any methods threw an error for relevant metrics).
rowSums(sapply(null10, function(x) colSums(is.na(assays(x)$qvalue)) > 0))

res10 <- plotsim_standardize(null10, alpha = seq(0.01, 0.10, 0.01))

plotsim_average(res10, met="rejections",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 
plotsim_average(res10, met="TNR",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 

covariateLinePlot(null10, alpha=0.05, covname="log2FC", nbins=25, 
                 trans="log1p")
covariateLinePlot(null10, alpha=0.05, covname="ind_covariate", nbins=25, 
                 trans="log1p")

if (numberMethodsReject(res10, alphacutoff=ualpha, filterSet=excludeSet) >= 2){
  aggupset(null10, alpha=ualpha, supplementary = FALSE, return_list = FALSE) 
}else{
  message("Not enough methods found rejections at alpha ", ualpha, 
          "; skipping upset plot")
}

```



## Sim D5: DE Comparison 5v5

Here we'll repeat the above, but for a DE comparison 5 versus 5 Snf2 samples,
where the groups are selected randomly and 2000 DE genes are added. 
This will be done for 100 random splits.

### Generate a list of SB results objects

```{r d5-uc, results='hide', message=FALSE}
rseed <- 198
sampleSize <- 5
nDE <- 2000

if (!file.exists(resfile_d5_uninfCov)){
  de5 <- mclapply(X=1:B, FUN=simulateOneSplit, rseed=rseed, 
                  nDE=nDE, sampleSize=sampleSize, bd=bd,
                  uninformativeCovariate = TRUE, mc.cores=nCores)
  saveRDS(de5, file = resfile_d5_uninfCov)
}else{
  de5 <- readRDS(file = resfile_d5_uninfCov)
}
```

### Plot average results over replications

Plot results. 

```{r}
# Check for missing results (if any methods threw an error for relevant metrics).
rowSums(sapply(de5, function(x) colSums(is.na(assays(x)$qvalue)) > 0))

res5d <- plotsim_standardize(de5, alpha = seq(0.01, 0.10, 0.01))

plotsim_average(res5d, met="rejections",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 
plotsim_average(res5d, met="FDR",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 
plotsim_average(res5d, met="TPR",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 

covariateLinePlot(de5, alpha=0.05, covname="log2FC", nbins=25, 
                 trans="log1p")
covariateLinePlot(de5, alpha=0.05, covname="ind_covariate", nbins=25, 
                 trans="log1p")

if (numberMethodsReject(res5d, alphacutoff=ualpha, filterSet=excludeSet) >= 2){
  aggupset(de5, alpha=ualpha, supplementary = FALSE, return_list = FALSE) 
  
}else{
  message("Not enough methods found rejections at alpha ", ualpha, 
          "; skipping upset plot")
}

```

## Sim D10: DE Comparison 10v10

Here we'll repeat the above, but for a DE comparison 10 versus 10 Snf2 samples,
where the groups are selected randomly and 2000 DE genes are added.
This will be done for 100 random splits.

### Generate a list of SB results objects

```{r d10-uc, results='hide', message=FALSE}
rseed <- 961
sampleSize <- 10
nDE <- 2000

if(!file.exists(resfile_d10_uninfCov)){
  de10 <- mclapply(X=1:B, FUN=simulateOneSplit, rseed=rseed, 
                   nDE=nDE, sampleSize=sampleSize, bd=bd,
                   uninformativeCovariate = TRUE, mc.cores=nCores)
  saveRDS(de10, file = resfile_d10_uninfCov)
}else{
  de10 <- readRDS(file = resfile_d10_uninfCov)
}
```

### Plot average results over replications

Plot results.

```{r}
# Check for missing results (if any methods threw an error for relevant metrics).
rowSums(sapply(de10, function(x) colSums(is.na(assays(x)$qvalue)) > 0))

res10d <- plotsim_standardize(de10, alpha = seq(0.01, 0.10, 0.01))

plotsim_average(res10d, met="rejections",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 
plotsim_average(res10d, met="FDR",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 
plotsim_average(res10d, met="TPR",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 

covariateLinePlot(de10, alpha=0.05, covname="log2FC", nbins=25, 
                 trans="log1p")
covariateLinePlot(de10, alpha=0.05, covname="ind_covariate", nbins=25, 
                 trans="log1p")

if (numberMethodsReject(res10d, alphacutoff=ualpha, filterSet=excludeSet) >= 2){
  aggupset(de10, alpha=ualpha, supplementary = FALSE, return_list = FALSE) 
}else{
  message("Not enough methods found rejections at alpha ", ualpha, 
          "; skipping upset plot")
}

```

## Sample size comparison

Here we compare the method ranks for the different sample sizes at alpha = 0.10.

```{r}
# uninformative covariate
plotMethodRanks(c(resfile_d5_uninfCov, resfile_d10_uninfCov), 
                colLabels = c("DE 5", "DE 10"), 
                alpha = 0.10, xlab = "Comparison", 
                excludeMethods = NULL)
```


# Session information

```{r}
sessionInfo()
```
