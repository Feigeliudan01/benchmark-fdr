---
title: "Polyester RNA-seq simulation study"
author: "Stephanie Hicks"
output: 
    html_document:
        toc: true
        toc_float: true
        highlight: tango
        number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This is an analysis using [polyester](https://bioconductor.org/packages/release/bioc/html/polyester.html) Bioconductor package to simulate RNA-Seq reads. We will  
(1) implement null comparisons on samples simulated using polyester
with no differentially expressed (DE) genes, (2) compare FDR approaches 
on their ability to discover 'true positive' DE genes with log2 fold 
changes simulated from a normal distribution. 

In this Rmd, we will perform a Monte Carlo simulation study 
and average over the replicates in plots.

## Set up workspace

```{r workspace-setup, results='hide', message=FALSE, warning=FALSE}
# Load packages needed to simulate RNA-seq counts with polyester
library(SummarizedBenchmark)
library(zebrafishRNASeq) 
library(genefilter)
library(limma)
library(polyester)
library(DESeq2)
library(data.table)
library(magrittr)
library(dplyr)
library(ggplot2)
library(cowplot)
library(tibble)
library(ggthemes)

# load helper functions
for (f in list.files("../R", "\\.(r|R)$", full.names = TRUE)) {
    source(f)
}


# set up results directories 
resdir <- "../../results/RNAseq"
dir.create(resdir, showWarnings = FALSE)

# results files that will be generated below
resfile_n5 <- file.path(resdir, "polyester-results-null5.rds")
resfile_n10 <- file.path(resdir, "polyester-results-null10.rds")
resfile_d5 <- file.path(resdir, "polyester-results-de5.rds")
resfile_d10 <- file.path(resdir, "polyester-results-de10.rds")

# # set up parallel backend:
library(BiocParallel)
nCores <- 20
register(MulticoreParam(workers = nCores))
```


## Set up Polyester

We use the [polyester](https://bioconductor.org/packages/release/bioc/html/polyester.html) Bioconductor package to simulate RNA-Seq reads. The package requires a count matrix as input which estimates the mean variance relationship. Here we start with the [zebrafish](https://bioconductor.org/packages/release/data/experiment/html/zebrafishRNASeq.html) Bioconductor data package. 

Estimate mean and variance relationship

```{r estimate-parameters-NB}
data(zfGenes)
dim(zfGenes)

filter <- apply(zfGenes, 1, function(x) length(x[x>5])>=2)
counts <- as.matrix(zfGenes[filter,])

## Estimate the zero inflated negative binomial parameters
params = get_params(counts)
```


## Helper functions for simulation

Next, we'll simulate RNA-Seq samples, both with and without the 
addition of simulated DE genes. Here we'll create a function that we can use to 
run one replicate given sample size and number of DE gene settings. This will be
looped over many replications and results averaged over them.


```{r}
#' @param counts is the full count dataset
#' @param paramPolyester is the parameters from running polyester. 
#' Contains information on the mean-variance relationship used to 
#' simulate RNA-Seq counts. 
#' @param sampleSize is the number of samples in each condition
#' @param nDE is the number of DE genes
#' @param bd is the bench design object
simulateOneSplit <- function(X, counts, paramPolyester, 
                             nDE, sampleSize, bd){
  
  # things needed to simulate with Polyester
  groupPolyester = rep(c(-1,1),each=sampleSize)
  modPolyester = model.matrix(~-1 + groupPolyester)
  
  # things needed for differential testing using DESeq2
  groupDESeq = factor(rep(c(0,1), each=sampleSize))
  
  # NULL simulation
  if(nDE == 0) {
    truth <- rep(FALSE, nrow(counts)) 
    names(truth) <- rownames(counts)
    
    # log2FC are all 0 in null simulation
    log2FC = rep(0,nrow(counts))
    dat_alt = create_read_numbers(paramPolyester$mu, paramPolyester$fit,
                                  paramPolyester$p0, m=dim(counts)[1], 
                                  n=sampleSize*2, beta=cbind(log2FC), 
                                  mod=modPolyester)
  }
  
  # DE simulation
  if(nDE > 0){
    # pick random set of nDE genes to add signal to
    DE <- sample(1:nrow(counts), nDE)
    truth <- rep(FALSE, nrow(counts))
    truth[DE] <- TRUE
    names(truth) <- rownames(counts)
    
    # randomly sample a log2FC from truncated N(0,0.5) (greater than 0.5) 
    log2FC <- rep(0, nrow(counts))
    # log2FC[DE] <- rtruncnorm(nDE, 1, sd=.5) * sample(c(1,-1), size=nDE, replace=TRUE)
    log2FC[DE] <- c(rnorm(nDE, sd = 1), rep(0,nrow(counts)-nDE))
    dat_alt = create_read_numbers(paramPolyester$mu, paramPolyester$fit,
                                  paramPolyester$p0, m=dim(counts)[1], 
                                  n=sampleSize*2, beta=cbind(log2FC), 
                                  mod=modPolyester)
  }
  
  # run DESeq2
  dds_alt <- DESeqDataSetFromMatrix(countData=dat_alt, 
                                    colData = DataFrame(groupDESeq), 
                                    design=~groupDESeq)
  dds_alt <- DESeq(dds_alt, parallel = FALSE)
  res_alt <- results(dds_alt, independentFiltering = F)
  
  geneExp <- tbl_df(data.frame(geneName=rownames(counts),
                               pval=res_alt$pvalue, SE=res_alt$lfcSE, 
                               ind_covariate = res_alt$baseMean, 
                               effect_size=res_alt$log2FoldChange, 
                               test_statistic=res_alt$stat, qvalue=truth, 
                               pzero = rowSums(counts(dds_alt)==0)/
                                 ncol(counts(dds_alt))  ))
  
  # filter NAs and those with less than 50% expressed 
  geneExp <- geneExp %>% na.omit() %>% dplyr::filter(pzero < 0.5)
  
  # built Benchmark data
  sb <- bd %>% buildBench(data=geneExp, parallel = FALSE, 
                          truthCols = "qvalue", 
                          ftCols = "ind_covariate")
  rowData(sb)$log2FC <- geneExp$effect_size
  
  return(sb)
}

```

We'll also set some parameters that will be common to all simulations. These
include the number of replications, the bench design object, the set of 
methods to exclude in the results plots, and the alpha cutoff level to 
be used when plotting the aggregated Upset results.

```{r}
B <- 100
bd <- initializeBenchDesign() # only needs to be done once
excludeSet <- c("unadjusted", "bl-df02", "bl-df04", "bl-df05") 
ualpha <- 0.05
```

We also add in Scott's FDR Regression (both
`nulltype = "empirical"` and `nulltype = "theoretical"`)
since our test statistics are approximately t-distributed. 

```{r}
bd <- addBMethod(bd, "scott-theoretical",
                     FDRreg::FDRreg,
                     function(x) { x$FDR },
                     z = test_statistic,
                     features = model.matrix( ~  splines::bs(ind_covariate, df = 3) - 1),
                     nulltype = 'theoretical',
                     control = list(lambda = 0.01))
bd <- addBMethod(bd, "scott-empirical",
                     FDRreg::FDRreg,
                     function(x) { x$FDR },
                     z = test_statistic,
                     features = model.matrix( ~  splines::bs(ind_covariate, df = 3) - 1),
                     nulltype = 'empirical',
                     control = list(lambda = 0.01))
```

Here's a helper function to return the number of methods with rejections at
a particular alpha level (this helps us determine whether or not to plot the
aggregated upset plot - if there aren't at least 2 methods it will throw an
error, which is a problem for the null simulations).

```{r}
#' @param res standardized metric data.table generated using
#'        standardize_results.
#' @param alpha alpha cutoff
#' @param filterSet which methods to exclude from consideration 
numberMethodsReject <- function(res, alphacutoff, filterSet){
  res <- res %>% 
    filter(is.na(param.alpha) | (param.alpha == alphacutoff)) %>%
    filter(!(blabel %in% filterSet)) %>%
    filter(alpha == alphacutoff) %>%
    filter(performanceMetric == "rejections") %>%
    select(blabel, performanceMetric, value) %>%
    group_by(blabel) %>%
    summarize(mean_value = mean(value)) %>%
    filter(mean_value > 0)
  return(nrow(res))
}
```


# Sim N5: Null Comparison 5v5

Here we run a null comparison 5 versus 5 RNA-Seq samples. 
This will be done for 100 simulations.

## Generate a list of SB results objects

```{r, results='hide', message=FALSE}
sampleSize <- 5
nDE <- 0

if (!file.exists(resfile_n5)){
  null5 <- bplapply(X=1:B, FUN=simulateOneSplit, counts=counts,
                   paramPolyester=params, nDE=nDE, sampleSize=sampleSize, bd=bd)
  saveRDS(null5, file=resfile_n5)
} else {
  null5 <- readRDS(file=resfile_n5)
}
```

## Plot average results over replications

Plot results. 

```{r}
# Check for missing results (if any methods threw an error for relevant metrics).
rowSums(sapply(null5, function(x) colSums(is.na(assays(x)$qvalue)) > 0))

res5 <- plotsim_standardize(null5, alpha = seq(0.01, 0.10, 0.01))

# metrics = ""TPR"   "FPR"   "TNR"   "FNR"   "rejections"  "FWER"   "rejectprop"
plotsim_average(res5, met="rejections",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 
plotsim_average(res5, met="TNR",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 

covariateLinePlot(null5, alpha=0.05, covname="log2FC", nbins=25, 
                 trans="log1p")
covariateLinePlot(null5, alpha=0.05, covname="ind_covariate", nbins=25, 
                 trans="log1p")

if (numberMethodsReject(res5, alphacutoff=ualpha, filterSet=excludeSet) >= 3){
  aggupset(null5, alpha=ualpha, supplementary = FALSE, return_list = FALSE) 
} else {
  message("Not enough methods found rejections at alpha ", ualpha, 
          "; skipping upset plot")
}

```




# Sim N10: Null Comparison 10v10

Here we'll repeat the above, but for a null comparison 10 versus 10 samples. 
This will be done for 100 simulations.

## Generate a list of SB results objects

```{r, results='hide', message=FALSE}
sampleSize <- 10
nDE <- 0

if (!file.exists(resfile_n10)){
  null10 <- bplapply(X=1:B, FUN=simulateOneSplit, counts=counts,
                   paramPolyester=params, nDE=nDE, sampleSize=sampleSize, bd=bd)
  saveRDS(null10, file=resfile_n10)
} else {
  null10 <- readRDS(file=resfile_n10)
}
```

## Plot average results over replications

Plot results.

```{r}
# Check for missing results (if any methods threw an error for relevant metrics).
rowSums(sapply(null10, function(x) colSums(is.na(assays(x)$qvalue)) > 0))

res10 <- plotsim_standardize(null10, alpha = seq(0.01, 0.10, 0.01))

plotsim_average(res10, met="rejections",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 
plotsim_average(res10, met="TNR",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 

covariateLinePlot(null10, alpha=0.05, covname="log2FC", nbins=25, 
                 trans="log1p")
covariateLinePlot(null10, alpha=0.05, covname="ind_covariate", nbins=25, 
                 trans="log1p")

if (numberMethodsReject(res10, alphacutoff=ualpha, filterSet=excludeSet) >= 3){
  aggupset(null10, alpha=ualpha, supplementary = FALSE, return_list = FALSE) 
} else {
  message("Not enough methods found rejections at alpha ", ualpha, "; skipping upset plot")
}

```

# Sim D5: DE Comparison 5v5

Here we'll repeat the above, but for a DE comparison 5 versus 5 samples
and 1500 DE genes are added. This will be done for 100 simulations.

## Generate a list of SB results objects

```{r, results='hide', message=FALSE}
sampleSize <- 5
nDE <- 1500

if (!file.exists(resfile_d5)){
  de5 <- bplapply(X=1:B, FUN=simulateOneSplit, counts=counts,
                   paramPolyester=params, nDE=nDE, sampleSize=sampleSize, bd=bd)
  saveRDS(de5, file=resfile_d5)
} else {
  de5 <- readRDS(file=resfile_d5)
}
```

## Plot average results over replications

Plot results. 

```{r}
# Check for missing results (if any methods threw an error for relevant metrics).
rowSums(sapply(de5, function(x) colSums(is.na(assays(x)$qvalue)) > 0))

res5d <- plotsim_standardize(de5, alpha = seq(0.01, 0.20, 0.01))

plotsim_average(res5d, met="rejections",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 
plotsim_average(res5d, met="FDR",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 
plotsim_average(res5d, met="TPR",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 

covariateLinePlot(de5, alpha=0.05, covname="log2FC", nbins=25, 
                 trans="log1p")
covariateLinePlot(de5, alpha=0.05, covname="ind_covariate", nbins=25, 
                 trans="log1p")

if (numberMethodsReject(res5d, alphacutoff=ualpha, filterSet=excludeSet) >= 3){
  aggupset(de5, alpha=ualpha, supplementary = FALSE, return_list = FALSE) 
} else {
  message("Not enough methods found rejections at alpha ", ualpha, "; skipping upset plot")
}

```

# Sim D10: DE Comparison 10v10

Here we'll repeat the above, but for a DE comparison 10 versus 10 samples
and 1500 DE genes are added. This will be done for 100 simulations.

## Generate a list of SB results objects

```{r, results='hide', message=FALSE}
sampleSize <- 10
nDE <- 1500

if (!file.exists(resfile_d10)){
  de10 <- bplapply(X=1:B, FUN=simulateOneSplit, counts=counts,
                   paramPolyester=params, nDE=nDE, sampleSize=sampleSize, bd=bd)
  saveRDS(de10, file=resfile_d10)
} else {
  de10 <- readRDS(file=resfile_d10)
}

```

## Plot average results over replications

Plot results.

```{r}
# Check for missing results (if any methods threw an error for relevant metrics).
rowSums(sapply(de10, function(x) colSums(is.na(assays(x)$qvalue)) > 0))

res10d <- plotsim_standardize(de10, alpha = seq(0.01, 0.20, 0.01))

plotsim_average(res10d, met="rejections",filter_set = excludeSet, 
                merge_ihw = TRUE, errorBars=TRUE) 
plotsim_average(res10d, met="FDR",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 
plotsim_average(res10d, met="TPR",filter_set = excludeSet,
                merge_ihw = TRUE, errorBars=TRUE) 

covariateLinePlot(de10, alpha=0.05, covname="log2FC", nbins=25, 
                 trans="log1p")
covariateLinePlot(de10, alpha=0.05, covname="ind_covariate", nbins=25, 
                 trans="log1p")

if (numberMethodsReject(res10d, alphacutoff=ualpha, filterSet=excludeSet) >= 3){
  aggupset(de10, alpha=ualpha, supplementary = FALSE, return_list = FALSE) 
} else {
  message("Not enough methods found rejections at alpha ", ualpha, "; skipping upset plot")
}
```

# Sample size comparison

Here we compare the method ranks for the different sample sizes at alpha = 0.10.

```{r}
plotMethodRanks(c(resfile_d5, resfile_d10), 
                colLabels = c("DE 5", "DE 10"), 
                alpha = 0.10, xlab = "Comparison",
                excludeMethods = NULL)
```

# Session information

```{r}
sessionInfo()
```


