---
title: "Case Study: RNA-Seq Differential Analysis (mir200c knockdown)"
author: "Alejandro Reyes"
date: "3/16/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Summary

As a second RNA-seq dataset, we will test for differences in gene expression upon the knockout of the microRNA mir-200c [(Kim et al., 2013)](http://dx.doi.org/10.1038/nsmb.2701).  The raw fastq files can be found under the accession number `SRP030475`. As the number of samples is limited the experiment might be underpowered, as in most RNA-seq analysis. This is an experimental scenario that could benefit from power gaining from using modern FDR control methods.

# Workspace Setup

```{r, workspace-setup, results='hide', message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(DESeq2)
library(SummarizedBenchmark)
library(BiocParallel)
library(recount)

## load helper functions
for (f in list.files("../R", "\\.(r|R)$", full.names = TRUE)) {
    source(f)
}

## project data/results folders
datdir <- "data"
resdir <- "results"
dir.create(datdir, showWarnings = FALSE)
dir.create(resdir, showWarnings = FALSE)

## intermediary files we create below
count_file <- file.path(datdir, "rse_gene.Rdata")
result_file <- file.path(resdir, "mir200c-results.rds")
bench_file <- file.path(resdir, "mir200c-benchmark.rds")

## set up parallel backend
cores <- as.numeric(Sys.getenv("SLURM_NTASKS"))
multicoreParam <- SerialParam()
```


# Data Preparation

We will download the pre-processed gene level counts available through recount2.

```{r}
if (!file.exists(count_file)) {
    download_study('SRP030475', outdir = datdir)
}
load(count_file)
dsd <- scale_counts(rse_gene)
```

We next subset for samples containing the control samples and the samples where mir200c was knocked down. Recount2 downloads data as a RangeSummarizedExperiment object, so we convert this into a DESeqDataSet object. 

```{r}
dsd <- dsd[, grepl("WT|200c", colData(dsd)$title)]
colData(dsd)$mir200c <- factor(ifelse(grepl("WT", colData(dsd)$title), "WT", "KO"))
dsd <- as(dsd, "DESeqDataSet")
storage.mode(assays(dsd)[["counts"]]) <- "integer"
```

# Data Analysis

## Differential Testing

Then, we set the design parameter to test for differences in expression upon mir200c knockout and run DESeq2. Similarly to the previous dataset, we set the parameter `independentFiltering=FALSE`.

```{r}
if (file.exists(result_file)) {
    res <- readRDS(result_file)
} else {
    design(dsd) <- ~ mir200c
    dds <- DESeq(dsd)
    res <- results(dds, independentFiltering = FALSE) %>% 
        as.data.frame() %>%
        na.omit() %>% 
        dplyr::select(pvalue, baseMean, log2FoldChange, lfcSE, stat) %>%
        dplyr::rename(pval = pvalue,
                      ind_covariate = baseMean, 
                      effect_size = log2FoldChange,
                      SE = lfcSE, 
                      test_statistic = stat)
    saveRDS(res, file = result_file)
}
```

## Covariate Diagnostics

As with the GTEx example, the mean counts is an informative covariate.

### Mean Counts

```{r, meancov-diag-scatter, fig.width=4.5, fig.height=3.5}
rank_scatter(res, pvalue = "pval", covariate = "ind_covariate") +
    ggtitle("Mean coverage as independent covariate") +
    xlab("Mean Expression")
```

Similarly to the GTEx dataset, keeping all the tests results in a strange discreteness. This is removed once we filter very lowly expressed genes. For the first covariate bin, however, there is a strange behaviour in which the distribution seems a bit skewed towards larger p-values. 

```{r, meancov-diag-hist, fig.width=10, fig.height=3.2}
strat_hist(res, pvalue = "pval",
           covariate = "ind_covariate", maxy = 7.5)

res <- filter(res, ind_covariate > 1)
strat_hist(res, pvalue = "pval",
           covariate = "ind_covariate", maxy = 3)
```

## Multiple-Testing Correction

We use the common `BenchDesign` with the set of multiple testing correction methods already included.

```{r}
if (file.exists(bench_file)) {
    sb <- readRDS(bench_file)
} else {
    bd <- initializeBenchDesign(nmids = 10000)
    sb <- buildBench(bd, data = res, ftCols = "ind_covariate")
    saveRDS(sb, file = bench_file)
}
```

There are some warnings from both `BH` and `scott-empirical`. However, they do return results. I tried to increase the `nmids` parameter but did not help.

```{r}
head(assays(sb)[["bench"]])
```

## Benchmark Metrics

Scott empirical rejects many more hypothesis than the rest of the methods, followed by lfdr and IHW.

```{r}
assayNames(sb) <- "qvalue"
sb <- addDefaultMetrics(sb)
estimatePerformanceMetrics(sb, tidy = TRUE) %>%
    filter(performanceMetric == "rejections", alpha == 0.1) %>%
    select(blabel, performanceMetric, alpha, value) %>%
    arrange(desc(value))
```

```{r}
rejections_scatter(sb, supplementary = FALSE)
rejection_scatter_bins(sb, covariate = "ind_covariate",
                       bins = 4, supplementary = FALSE)
```

```{r}
plotFDRMethodsOverlap(sb, alpha = 0.05, nsets = ncol(sb),
                      order.by = "freq", decreasing = TRUE,
                      supplementary = FALSE)
```

```{r}
methods <- c("ashs", "lfdr", "ihw-a10", "bl-df03", "qvalue", "bh", "bonf", "scott-empirical")
plotCovariateBoxplots(sb, alpha = 0.1, nsets = 6,
                      methods = methods, trans = log2, maxNum = 5)
```

# Session Info

```{r}
sessionInfo()
```
