---
title: "Benchmarking FDR Methods - Simulation Summarized ROC / UpsetR Plots"
author: "Rafalab Journal Club Members"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    highlight: tango
    number_sections: true
    code_folding: hide
---

```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, fig.width = 14, fig.height = 6)
```

<!-- ######################################################################## -->
## Workspace Setup
<!-- ######################################################################## -->

```{r, load-libraries, message=FALSE}
library("SummarizedBenchmark")
library("tidyverse")
```

```{r, helper-functions, message=FALSE}
source("../R/plotsim_aggupset.R")
```

In contrast to the t-test simulations, underlying data was not simulated in this series of simulations. Instead, for each simulation, a collection (20,000) of test statistics
were directly sampled from univariate distributions (Normal, non-central-t, or non-central-chi-squared). Test statistics corresponding to null tests are sampled with non-centrality
parameter zero, and non-null tests are sampled with some non-zero non-centrality parameter, determined based on the simulation setting. In these simulations, the magnitude of
the non-centrality parameter for each test can be understood as the effect size or signal in that test. The univariate distributions (Normal, t, chi-squared) are the variyng
sampling distributions that may be encountered under different testing scenarios (e.g. Student's t-test or Pearson's goodness-of-fit).

Since ASH requires standard errors, the true standard errors is provided to the method.

All simulation settings were carried out with an independent covariate. Both settings where the covariate was informative and uninformative were considered. Additionally, several different informative covariates were considered. Following the approach of Boca and Leek (2015) and Chen, Robinson and Storey (2017), we sampled the independent covariate for each test uniformly from the interval [0, 1]. Whether the covariate was informative or uninformative was determined by a functional relationship between the value of the covariate and the probability of the corresponding test being null. The following relationships were considered:

- pi0(x) = pi0 (`uniform-80`)
- pi0(x) = pi0 (`uniform-90`)
- pi0(x) = pi0 (`uniform-95`)
- pi0(x) = 0.25*I(x < 0.25) + ... (`step-more`, 80% pi0)
- pi0(x) = 0.25*I(x < 0.25) + ... (`step-90`, 90% pi0)
- pi0(x) = 0.25*I(x < 0.25) + ... (`step-95`, 95% pi0)
- pi0(x) = 1 - x^3 ... (`cubic`, 80% pi0)
- pi0(x) = 1 - x^3 ... (`cubic-90`, 90% pi0)
- pi0(x) = 1 - x^3 ... (`cubic-95`, 95% pi0)

Here, we only consider a subset of the entire set of simulation settings. Namely, we look at: [1] null simulations, [2] non-null simulations with an informative covariate (cubic), and [3] non-null simulations with an uninformative covariate. For each of the 3 simulation sets, we further consider three different sampling distributions of the test statistics (Normal, t, chi-squared).

Performance is evaluated with: the false positive rate (FPR), true positive rate (TPR), and method overlaps (upset plots). For null simulations, since FPR and TPR are useless, we instead plot the proportion of rejected tests.

<!-- ######################################################################## -->
## Local Helper Functions
<!-- ######################################################################## -->

```{r}
preclean <- function(res, merge_ihw = TRUE, supplementary = FALSE) {
    ## set of method names
    method_names <- unique(res$blabel)
    ## remove unadjusted p-values - too extreme
    if ("unadjusted" %in% method_names) {
        res <- res %>%
            filter(blabel != "unadjusted")
    }
    ## drop ihw not matching 'alpha'
    if (merge_ihw) {
        res <- res %>%
            mutate(ihw_alpha = gsub("ihw-a(.*)", "\\1", blabel),
                   ihw_alpha = ifelse(grepl("^ihw-a", blabel), ihw_alpha, alpha*100),
                   ihw_alpha = as.numeric(ihw_alpha) / 100) %>%
            filter(abs(alpha - ihw_alpha) < 1e-6) %>%
            mutate(blabel = ifelse(grepl("^ihw-a", blabel), "ihw", blabel))
    }
    ## drop excess Boca-Leek results
    if(!supplementary) {
        methods_bl <- grep("^bl-", method_names, value = TRUE)
        res <- res %>%
            filter(!(blabel %in% setdiff(methods_bl, "bl-df03")))
    }
    return(res)
}

myplot <- function(res, metrics = c("TPR", "FPR"), alpha_max = 1) {
    gp <- res %>%
        filter(performanceMetric %in% metrics) %>%
        filter (alpha <= alpha_max) %>%
        group_by(alpha, blabel, performanceMetric) %>%
        summarize(value50 = median(value, na.rm = TRUE),
                  value10 = quantile(value, 0.10, na.rm = TRUE),
                  value90 = quantile(value, 0.90, na.rm = TRUE)) %>%
        ungroup %>%
        ggplot(aes(x = alpha, y = value50, ymin = value10, ymax = value90,
                   group = blabel, color = blabel))
    if ("FPR" %in% metrics) {
        gp <- gp +
            geom_abline(aes(intercept = y, slope = s), alpha = 1/2,
                        data = data.frame(y = 0, s = 1, performanceMetric = "FPR"))
    }
    gp <- gp +
        geom_line(alpha = 1/4,
                  position = position_dodge(width = 0.003)) +
        geom_errorbar(width = .01,
                      position = position_dodge(width = 0.003)) + 
        geom_point(alpha = 1/2, position = position_dodge(width = 0.003)) +
        facet_wrap(~ performanceMetric) +
        scale_color_discrete("Method") + 
        ylab("performance across replicates") +
        xlab("alpha cutoff") +
        theme_bw()
    gp 
}
```

<!-- ######################################################################## -->
## Null Simulations
<!-- ######################################################################## -->

First, we look at the null simulation settings to assess which methods control false discoveries when no truly significant tests are present. The TPR is not reported here because there are no "true positivies" in the null setting.

- *null* (pi0 = 100) simulations across *noise distributions*
  1. *null* (pi0 = 100) simulations across *Gaussian*
  2. *null* (pi0 = 100) simulations across *t*
  3. *null* (pi0 = 100) simulations across *chi-squared*

### Normal

```{r, fig.width = 7, fig.height = 6}
## common suffix 
rds_name <- "M100/allnull-uniform/results-altnoise_normal_null-M100.rds"

## read in performance metrics, plot metric
mets <- readRDS(file.path("data-psim-parsed", rds_name))
mets <- preclean(mets)
myplot(mets, metrics = "rejectprop") + 
    ggtitle("Distribution: normal, pi0: 100% (100 replications)")
```

### t (10 dof)

```{r, fig.width = 7, fig.height = 6}
## common suffix 
rds_name <- "M100/allnull-uniform/results-altnoise_t_df10_null-M100.rds"

## read in performance metrics, plot metric
mets <- readRDS(file.path("data-psim-parsed", rds_name))
mets <- preclean(mets)
myplot(mets, metrics = "rejectprop") +
    ggtitle("Distribution: t (10 dof), pi0: 100% (100 replications)")
```

### chi-sq (4 dof)

```{r, fig.width = 7, fig.height = 6}
## common suffix 
rds_name <- "M100/allnull-uniform/results-altnoise_chisq_df4_null-M100.rds"

## read in performance metrics, plot metric
mets <- readRDS(file.path("data-psim-parsed", rds_name))
mets <- preclean(mets)
myplot(mets, metrics = "rejectprop") +
    ggtitle("Distribution: chi-squared (4 dof), pi0: 100% (100 replications)")
```


<!-- ######################################################################## -->
## Informative Covariate Simulations
<!-- ######################################################################## -->

Next, we consider simulations where a truly informative covariate is present.

- *cubic-informative*, *non-null* (pi0 = 80,90,95) simulations across *noise distributions*
  1. *cubic-informative*, *non-null* (pi0 = 80,90,95) simulations across *Gaussian*
  2. *cubic-informative*, *non-null* (pi0 = 80,90,95) simulations across *t*
  3. *cubic-informative*, *non-null* (pi0 = 80,90,95) simulations across *chi-squared*

### Normal

```{r}
## common suffix 
rds_name <- "M100/esize_fixed-bl-cubic-90-0.8/results-alteff_30-M100.rds"

## read in performance metrics, plot metric
mets <- readRDS(file.path("data-psim-parsed", rds_name))
mets <- preclean(mets)
myplot(mets) + 
    ggtitle("Distribution: normal, pi0: 90% (100 replications)")

## read in SB objects, plot aggregate upset plot
res <- readRDS(file.path("data-psim", rds_name))
aggupset(res, alpha = 0.05)
```

Also looking at varying pi0s (only showing FPR/TPR).

```{r}
"M100/esize_fixed-bl-cubic/results-alteff_30-M100.rds" %>%
    file.path("data-psim-parsed", .) %>%
    readRDS() %>%
    preclean() %>%
    myplot() + 
    ggtitle("Distribution: normal, pi0: 80% (100 replications)")

"M100/esize_fixed-bl-cubic-95-0.8/results-alteff_30-M100.rds" %>%
    file.path("data-psim-parsed", .) %>%
    readRDS() %>%
    preclean() %>%
    myplot() + 
    ggtitle("Distribution: normal, pi0: 95% (100 replications)")
```

### t (10 dof)

```{r}
rds_name <- "M100/altnoise-bl-cubic-90-0.8/results-altnoise_t_df10_bimodal-M100.rds"

## read in performance metrics, plot metric
mets <- readRDS(file.path("data-psim-parsed", rds_name))
mets <- preclean(mets)
myplot(mets) +
    ggtitle("Distribution: t (10 dof), pi0: 90% (100 replications)")

## read in SB objects, plot aggregate upset plot
res <- readRDS(file.path("data-psim", rds_name))
aggupset(res, alpha = 0.05)
```

Also looking at varying pi0s (only showing FPR/TPR).

```{r}
"M100/altnoise-bl-cubic/results-altnoise_t_df10_bimodal-M100.rds" %>%
    file.path("data-psim-parsed", .) %>%
    readRDS() %>%
    preclean() %>%
    myplot() + 
    ggtitle("Distribution: t (10 dof), pi0: 80% (100 replications)")

"M100/altnoise-bl-cubic-95-0.8/results-altnoise_t_df10_bimodal-M100.rds" %>%
    file.path("data-psim-parsed", .) %>%
    readRDS() %>%
    preclean() %>%
    myplot() + 
    ggtitle("Distribution: t (10 dof), pi0: 95% (100 replications)")
```

### chi-sq (4 dof)

```{r}
## common suffix 
rds_name <- "M100/altnoise-bl-cubic-90-0.8/results-altnoise_chisq_df4_shift3sq-M100.rds"

## read in performance metrics, plot metric
mets <- readRDS(file.path("data-psim-parsed", rds_name))
mets <- preclean(mets)
myplot(mets) +
    ggtitle("Distribution: chi-squared (4 dof), pi0: 90% (100 replications)")

## read in SB objects, plot aggregate upset plot
res <- readRDS(file.path("data-psim", rds_name))
aggupset(res, alpha = 0.05)
```

Also looking at varying pi0s (only showing FPR/TPR).

```{r}
"M100/altnoise-bl-cubic/results-altnoise_chisq_df4_shift3sq-M100.rds" %>%
    file.path("data-psim-parsed", .) %>%
    readRDS() %>%
    preclean() %>%
    myplot() + 
    ggtitle("Distribution: chi-squared (4 dof), pi0: 80% (100 replications)")

"M100/altnoise-bl-cubic-95-0.8/results-altnoise_chisq_df4_shift3sq-M100.rds" %>%
    file.path("data-psim-parsed", .) %>%
    readRDS() %>%
    preclean() %>%
    myplot() + 
    ggtitle("Distribution: chi-squared (4 dof), pi0: 95% (100 replications)")
```

<!-- ######################################################################## -->
## Uninformative Covariate Simulations
<!-- ######################################################################## -->

Finally, we consider simulations with non-null (alternative) tests are present. 

- *non-informative*, *non-null* (pi0 = 80,90,95) simulations across *noise distributions*
  1. *uninformative*, *non-null* (pi0 = 80,90,95) simulations across *Gaussian*
  2. *uninformative*, *non-null* (pi0 = 80,90,95) simulations across *t*
  3. *uninformative*, *non-null* (pi0 = 80,90,95) simulations across *chi-squared*

### Normal

```{r}
## common suffix 
rds_name <- "M100/esize_fixed-uniform-0.9/results-alteff_30-M100.rds"

## read in performance metrics, plot metric
mets <- readRDS(file.path("data-psim-parsed", rds_name))
mets <- preclean(mets)
myplot(mets) + 
    ggtitle("Distribution: normal, pi0: 90% (100 replications)")

## read in SB objects, plot aggregate upset plot
res <- readRDS(file.path("data-psim", rds_name))
aggupset(res, alpha = 0.05)
```

Also looking at varying pi0s (only showing FPR/TPR).

```{r}
"M100/esize_fixed-uniform/results-alteff_30-M100.rds" %>%
    file.path("data-psim-parsed", .) %>%
    readRDS() %>%
    preclean() %>%
    myplot() + 
    ggtitle("Distribution: normal, pi0: 80% (100 replications)")

"M100/esize_fixed-bl-uniform-0.95/results-alteff_30-M100.rds" %>%
    file.path("data-psim-parsed", .) %>%
    readRDS() %>%
    preclean() %>%
    myplot() + 
    ggtitle("Distribution: normal, pi0: 95% (100 replications)")

"M100/esize_fixed-bl-uniform-0.99/results-alteff_30-M100.rds" %>%
    file.path("data-psim-parsed", .) %>%
    readRDS() %>%
    preclean() %>%
    myplot() + 
    ggtitle("Distribution: normal, pi0: 99% (100 replications)")
```

### t (10 dof)

```{r}
rds_name <- "M100/altnoise-uniform/results-altnoise_t_df10_bimodal-M100.rds"

## read in performance metrics, plot metric
mets <- readRDS(file.path("data-psim-parsed", rds_name))
mets <- preclean(mets)
myplot(mets) +
    ggtitle("Distribution: t (10 dof), pi0: 80% (100 replications)")

## read in SB objects, plot aggregate upset plot
res <- readRDS(file.path("data-psim", rds_name))
aggupset(res, alpha = 0.05)
```

*Need to run with different pi0 values.*

### chi-sq (4 dof)

```{r}
rds_name <- "M100/altnoise-uniform/results-altnoise_chisq_df4_shift3sq-M100.rds"

## read in performance metrics, plot metric
mets <- readRDS(file.path("data-psim-parsed", rds_name))
mets <- preclean(mets)
myplot(mets) +
    ggtitle("Distribution: chi-squared (4 dof), pi0: 80% (100 replications)")

## read in SB objects, plot aggregate upset plot
res <- readRDS(file.path("data-psim", rds_name))
aggupset(res, alpha = 0.05)
```

*Need to run with different pi0 values.*

