---
title: "Simulation Study: Varying Effect Size"
author: "Patrick Kimes"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: 
   html_document:
        toc: true
        toc_float: true
        highlight: tango
        number_sections: true
---

# Summary

In this set of simulations, we consider settings with both null and non-null
tests with varying distribution of effect sizes under the non-null (alternative)
setting. An informative covariate is included in the setting as described in
`simulations-informative-sine.Rmd`. 


# Workspace Setup

```{r, wkspace-setup, results='hide', message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(SummarizedBenchmark)
library(parallel)

## load helper functions
for (f in list.files("../R", "\\.(r|R)$", full.names = TRUE)) {
    source(f)
}

## project data/results folders
resdir <- "results"
dir.create(resdir, showWarnings = FALSE, recursive = TRUE)

## intermediary files we create below
spiky_file <- file.path(resdir, "varyinges-benchmark-spiky.rds")
flattop_file <- file.path(resdir, "varyinges-benchmark-flattop.rds")
skew_file <- file.path(resdir, "varyinges-benchmark-skew.rds")
bimodal_file <- file.path(resdir, "varyinges-benchmark-bimodal.rds")

## number of cores for parallelization
cores <- 20
B <- 100

## define bechmarking design
bd <- initializeBenchDesign()
```

As described in `simulations-null.Rmd`, we include Scott's FDR Regression in the analysis
for simulations with Gaussian or t-distributed noise. Again, we include both
`nulltype = "empirical"` and `nulltype = "theoretical"`. Since all settings in this
series of simulations use test statistics simulated with Gaussian noise, we include
Scott's FDR Regression in all of the comparisons.

```{r}
bdplus <- bd
bdplus <- addBMethod(bdplus, "scott-theoretical",
                     FDRreg::FDRreg,
                     function(x) { x$FDR },
                     z = test_statistic,
                     features = model.matrix( ~  splines::bs(ind_covariate, df = 3) - 1),
                     nulltype = 'theoretical',
                     control = list(lambda = 0.01))
bdplus <- addBMethod(bdplus, "scott-empirical",
                     FDRreg::FDRreg,
                     function(x) { x$FDR },
                     z = test_statistic,
                     features = model.matrix( ~  splines::bs(ind_covariate, df = 3) - 1),
                     nulltype = 'empirical',
                     control = list(lambda = 0.01))
```

All simulation settings will share the following parameters.

```{r parameters-shared}
m <- 20000                          # integer: number of hypothesis tests
pi0 <- pi0_sine(0.90)               # numeric: proportion of null hypotheses
tstat_dist <- rnorm_perturber(1/2)  # functional: sampling dist/noise for test stats
null_dist <- rnorm_2pvaluer(1/2)    # functional: dist to calc p-values
icovariate <- runif                 # functional: independent covariate
```

Simulation results will be presented excluding a subset of methods, and
for certain plots (upset plots), a single alpha cutoff will be used.

```{r}
excludeSet <- c("unadjusted", "bl-df02", "bl-df04", "bl-df05")
ualpha <- 0.05
```

# Spiky Setting

First, we consider the setting where the effect sizes under the alternative are
distributed according to a "spiky" unimodal distribution centered around zero, as
defined in the ASH simulations.

## Data Simulation

```{r spiky-parameters}
tstat <- sampler_spiky       # functional: dist of alternative test stats
seed <- 778
```

We next run the simulations.

```{r spiky-run-simulation}
if (file.exists(spiky_file)) {
    res <- readRDS(spiky_file)
} else {
    res <- mclapply(X = 1:B, FUN = simIteration, bench = bdplus, m = m,
                    pi0 = pi0, tstat = tstat, icovariate = icovariate,
                    tstat_dist = tstat_dist, null_dist = null_dist,
                    seed = seed, mc.cores = cores)
    saveRDS(res, file = spiky_file)
}
```

## Covariate Diagnostics

Here, we show the relationship between the independent covariate and p-values for a
single replication of the experiment.

```{r spiky-one-simulation}
onerun <- simIteration(bdplus, m = m, pi0 = pi0, tstat = tstat, tstat_dist = tstat_dist,
                       icovariate = icovariate, null_dist = null_dist, execute = FALSE)
```

```{r, spiky-diag-scatter, results = "hide", fig.width=4.5, fig.height=3.5}
rank_scatter(onerun, pvalue = "pval", covariate = "ind_covariate")
```

```{r, spiky-diag-hist, results = "hide", fig.width=10, fig.height=3.2}
strat_hist(onerun, pvalue = "pval", covariate = "ind_covariate", maxy = 5, numQ = 3)
```

## Benchmark Metrics

We plot the averaged results across `r B` replications.

```{r spiky-metrics-averages, results = "hide"}
resdf <- plotsim_standardize(res, alpha = seq(0.01, 0.10, 0.01))

plotsim_average(resdf, met="rejections", filter_set = excludeSet,
                merge_ihw = TRUE, errorBars = TRUE) 

plotsim_average(resdf, met="FDR", filter_set = excludeSet,
                merge_ihw = TRUE, errorBars = TRUE) 

plotsim_average(resdf, met="TPR", filter_set = excludeSet,
                merge_ihw = TRUE, errorBars = TRUE) 

plotsim_average(resdf, met="TNR", filter_set = excludeSet,
                merge_ihw = TRUE, errorBars = TRUE) 
```

We also take a look at the distribution of rejects for each method as a function of
the effect size and independent covariate.

```{r spiky-metrics-covlineplot, results = "hide"}
covariateLinePlot(res, alpha = ualpha, covname = "effect_size")

covariateLinePlot(res, alpha = ualpha, covname = "ind_covariate")
```

Finally, (if enough methods produce rejections at `r ualpha`) we take a look at
the overlap of rejections between methods.

```{r spiky-metrics-upset, results = "hide"}
if (numberMethodsReject(resdf, alphacutoff = ualpha, filterSet = excludeSet) >= 3) {
    aggupset(res, alpha = ualpha, supplementary = FALSE, return_list = FALSE)
} else {
    message("Not enough methods found rejections at alpha ", ualpha, 
            "; skipping upset plot")
}
```

# Flat-Top Setting

Next, we consider the setting where the effect sizes under the alternative are
distributed according to a "flat top" unimodal distribution centered around zero, as
defined in the ASH simulations.

## Data Simulation

```{r flattop-parameters}
tstat <- sampler_flat_top       # functional: dist of alternative test stats
seed <- 980
```

We next run the simulations.

```{r flattop-run-simulation}
if (file.exists(flattop_file)) {
    res <- readRDS(flattop_file)
} else {
    res <- mclapply(X = 1:B, FUN = simIteration, bench = bdplus, m = m,
                    pi0 = pi0, tstat = tstat, icovariate = icovariate,
                    tstat_dist = tstat_dist, null_dist = null_dist,
                    seed = seed, mc.cores = cores)
    saveRDS(res, file = flattop_file)
}
```

## Covariate Diagnostics

Here, we show the relationship between the independent covariate and p-values for a
single replication of the experiment.

```{r flattop-one-simulation}
onerun <- simIteration(bdplus, m = m, pi0 = pi0, tstat = tstat, tstat_dist = tstat_dist,
                       icovariate = icovariate, null_dist = null_dist, execute = FALSE)
```

```{r, flattop-diag-scatter, results = "hide", fig.width=4.5, fig.height=3.5}
rank_scatter(onerun, pvalue = "pval", covariate = "ind_covariate")
```

```{r, flattop-diag-hist, results = "hide", fig.width=10, fig.height=3.2}
strat_hist(onerun, pvalue = "pval", covariate = "ind_covariate", maxy = 5, numQ = 3)
```

## Benchmark Metrics

We plot the averaged results across `r B` replications.

```{r flattop-metrics-averages, results = "hide"}
resdf <- plotsim_standardize(res, alpha = seq(0.01, 0.10, 0.01))

plotsim_average(resdf, met="rejections", filter_set = excludeSet,
                merge_ihw = TRUE, errorBars = TRUE) 

plotsim_average(resdf, met="FDR", filter_set = excludeSet,
                merge_ihw = TRUE, errorBars = TRUE) 

plotsim_average(resdf, met="TPR", filter_set = excludeSet,
                merge_ihw = TRUE, errorBars = TRUE) 

plotsim_average(resdf, met="TNR", filter_set = excludeSet,
                merge_ihw = TRUE, errorBars = TRUE) 
```

We also take a look at the distribution of rejects for each method as a function of
the effect size and independent covariate.

```{r flattop-metrics-covlineplot, results = "hide"}
covariateLinePlot(res, alpha = ualpha, covname = "effect_size")

covariateLinePlot(res, alpha = ualpha, covname = "ind_covariate")
```

Finally, (if enough methods produce rejections at `r ualpha`) we take a look at
the overlap of rejections between methods.

```{r flattop-metrics-upset, results = "hide"}
if (numberMethodsReject(resdf, alphacutoff = ualpha, filterSet = excludeSet) >= 3) {
    aggupset(res, alpha = ualpha, supplementary = FALSE, return_list = FALSE)
} else {
    message("Not enough methods found rejections at alpha ", ualpha, 
            "; skipping upset plot")
}
```

# Skewed Setting

Next, we consider the setting where the effect sizes under the alternative are
distributed according to a skewed unimodal distribution not centered at zero, as
defined in the ASH simulations.

## Data Simulation

```{r skew-parameters}
tstat <- sampler_skew       # functional: dist of alternative test stats
seed <- 206
```

We next run the simulations.

```{r skew-run-simulation}
if (file.exists(skew_file)) {
    res <- readRDS(skew_file)
} else {
    res <- mclapply(X = 1:B, FUN = simIteration, bench = bdplus, m = m,
                    pi0 = pi0, tstat = tstat, icovariate = icovariate,
                    tstat_dist = tstat_dist, null_dist = null_dist,
                    seed = seed, mc.cores = cores)
    saveRDS(res, file = skew_file)
}
```

## Covariate Diagnostics

Here, we show the relationship between the independent covariate and p-values for a
single replication of the experiment.

```{r skew-one-simulation}
onerun <- simIteration(bdplus, m = m, pi0 = pi0, tstat = tstat, tstat_dist = tstat_dist,
                       icovariate = icovariate, null_dist = null_dist, execute = FALSE)
```

```{r, skew-diag-scatter, results = "hide", fig.width=4.5, fig.height=3.5}
rank_scatter(onerun, pvalue = "pval", covariate = "ind_covariate")
```

```{r, skew-diag-hist, results = "hide", fig.width=10, fig.height=3.2}
strat_hist(onerun, pvalue = "pval", covariate = "ind_covariate", maxy = 5, numQ = 3)
```

## Benchmark Metrics

We plot the averaged results across `r B` replications.

```{r skew-metrics-averages, results = "hide"}
resdf <- plotsim_standardize(res, alpha = seq(0.01, 0.10, 0.01))

plotsim_average(resdf, met="rejections", filter_set = excludeSet,
                merge_ihw = TRUE, errorBars = TRUE) 

plotsim_average(resdf, met="FDR", filter_set = excludeSet,
                merge_ihw = TRUE, errorBars = TRUE) 

plotsim_average(resdf, met="TPR", filter_set = excludeSet,
                merge_ihw = TRUE, errorBars = TRUE) 

plotsim_average(resdf, met="TNR", filter_set = excludeSet,
                merge_ihw = TRUE, errorBars = TRUE) 
```

We also take a look at the distribution of rejects for each method as a function of
the effect size and independent covariate.

```{r skew-metrics-covlineplot, results = "hide"}
covariateLinePlot(res, alpha = ualpha, covname = "effect_size")

covariateLinePlot(res, alpha = ualpha, covname = "ind_covariate")
```

Finally, (if enough methods produce rejections at `r ualpha`) we take a look at
the overlap of rejections between methods.

```{r skew-metrics-upset, results = "hide"}
if (numberMethodsReject(resdf, alphacutoff = ualpha, filterSet = excludeSet) >= 3) {
    aggupset(res, alpha = ualpha, supplementary = FALSE, return_list = FALSE)
} else {
    message("Not enough methods found rejections at alpha ", ualpha, 
            "; skipping upset plot")
}
```

# Bimodal Setting

Finally, we consider the setting where the effect sizes under the alternative are
distributed according to a bimodal distribution (equal mixture of two normal distributions
centered at -2, 2, with variance 1), again, as defined in the ASH simulations.

## Data Simulation

```{r bimodal-parameters}
tstat <- sampler_bimodal       # functional: dist of alternative test stats
seed <- 913
```

We next run the simulations.

```{r bimodal-run-simulation}
if (file.exists(bimodal_file)) {
    res <- readRDS(bimodal_file)
} else {
    res <- mclapply(X = 1:B, FUN = simIteration, bench = bdplus, m = m,
                    pi0 = pi0, tstat = tstat, icovariate = icovariate,
                    tstat_dist = tstat_dist, null_dist = null_dist,
                    seed = seed, mc.cores = cores)
    saveRDS(res, file = bimodal_file)
}
```

## Covariate Diagnostics

Here, we show the relationship between the independent covariate and p-values for a
single replication of the experiment.

```{r bimodal-one-simulation}
onerun <- simIteration(bdplus, m = m, pi0 = pi0, tstat = tstat, tstat_dist = tstat_dist,
                       icovariate = icovariate, null_dist = null_dist, execute = FALSE)
```

```{r, bimodal-diag-scatter, results = "hide", fig.width=4.5, fig.height=3.5}
rank_scatter(onerun, pvalue = "pval", covariate = "ind_covariate")
```

```{r, bimodal-diag-hist, results = "hide", fig.width=10, fig.height=3.2}
strat_hist(onerun, pvalue = "pval", covariate = "ind_covariate", maxy = 5, numQ = 3)
```

## Benchmark Metrics

We plot the averaged results across `r B` replications.

```{r bimodal-metrics-averages, results = "hide"}
resdf <- plotsim_standardize(res, alpha = seq(0.01, 0.10, 0.01))

plotsim_average(resdf, met="rejections", filter_set = excludeSet,
                merge_ihw = TRUE, errorBars = TRUE) 

plotsim_average(resdf, met="FDR", filter_set = excludeSet,
                merge_ihw = TRUE, errorBars = TRUE) 

plotsim_average(resdf, met="TPR", filter_set = excludeSet,
                merge_ihw = TRUE, errorBars = TRUE) 

plotsim_average(resdf, met="TNR", filter_set = excludeSet,
                merge_ihw = TRUE, errorBars = TRUE) 
```

We also take a look at the distribution of rejects for each method as a function of
the effect size and independent covariate.

```{r bimodal-metrics-covlineplot, results = "hide"}
covariateLinePlot(res, alpha = ualpha, covname = "effect_size")

covariateLinePlot(res, alpha = ualpha, covname = "ind_covariate")
```

Finally, (if enough methods produce rejections at `r ualpha`) we take a look at
the overlap of rejections between methods.

```{r bimodal-metrics-upset, results = "hide"}
if (numberMethodsReject(resdf, alphacutoff = ualpha, filterSet = excludeSet) >= 3) {
    aggupset(res, alpha = ualpha, supplementary = FALSE, return_list = FALSE)
} else {
    message("Not enough methods found rejections at alpha ", ualpha, 
            "; skipping upset plot")
}
```
