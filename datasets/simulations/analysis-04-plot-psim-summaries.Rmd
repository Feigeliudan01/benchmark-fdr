---
title: "Benchmarking FDR Methods - Summarizing Simulation Results (Part II)"
author: "Rafalab Journal Club Members"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    highlight: tango
    number_sections: true
    code_folding: hide
---

```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, fig.width = 12, fig.height = 10)
```

<!-- ######################################################################## -->
## Workspace Setup
<!-- ######################################################################## -->

```{r, load-libraries, message=FALSE}
library("data.table")
library("SummarizedBenchmark")
library("tidyverse")
library("magrittr")
library("forcats")
```

```{r, load-helper-functions}
source("../R/plotsim_standardize.R")
```

```{r, process-data, message=FALSE}
## list of results from 100 replication experiments
dl <- list.dirs("data-psim/M100", recursive=FALSE, full.names=TRUE)

## parse each setting-block separately and save metrics in single file
for (dd in dl) {
    if (!file.exists(file.path(dd, "results-performanceMetrics-alpha05.rds"))) {
        print(paste0("running: ", basename(dd))) 
        fl <- list.files(dd, pattern=".rds", full.names=TRUE)
        tsbl <- list()
        for (f in fl) {
            ## read process data
            res <- readRDS(f)
            tsb <- plotsim_standardize(res, alpha = 0.05)
            tsbs <- tsb %>%
                filter(!(blabel %in% c("bl-df02", "bl-df04", "bl-df05", 
                                       paste0("ihw-a0", c(1:4, 6:9)), "ihw-a10")),
                       alpha == 0.05) %>%
                select(blabel, performanceMetric, value, rep)
            tsbl[[gsub(".*/(.*)\\.rds", "\\1", f)]] <- tsbs
        }
        tsbc <- as.tibble(bind_rows(tsbl, .id = "param"))
        print(paste0("saving: ", basename(dd))) 
        saveRDS(tsbc, file.path(dd, "results-performanceMetrics-alpha05.rds"))
        print(paste0("done: ", basename(dd))) 
    } else {
        print(paste0("skipping: ", basename(dd)))
    }
}
```

<!-- ######################################################################## -->
## Overview
<!-- ######################################################################## -->

This analysis is similar to the one presented in `analysis-03-plot-tsim-summaries.[Rmd|html]`. However, rather than looking at the results of two-sample t-tests, here, we consider simulations performed directly at the level of effect sizes and p-values without any underlying data. This approach to simulating data follows similar simulations carried out in the Boca-Leek, IHW ("size investing strategy" simulations), and ASH papers. Different simulation settings were considered to evaluate the impact of varying effect sizes, effect size distributions, sampling distributions, proportions of null hypotheses, and presence or absence of an informative covariate. By definition, the ASH method makes use of the standard error (SE) for each hypothesis test, and does not accept any other additional information. The results for the ASH method are therefore always based on using the SE as the "informative covariate." More details on the simulation settings considered are included in the header of the `psim-core-settings.R` script. Each setting was replicated 100 times.

Since these simulations do not rely on underlying simulated data, but rather simulate summary statistics directly, there is no way of calculating a corresponding SE for each hypothesis test. *As such, the ASH method, which requires the SE for each test, is run using the true (typically unknown) SE underlying the simulation.* Therefore, the ASH results must be interpreted with caution.

As in `analysis-03-plot-tsim-summaries.[Rmd|html]`, simulation results are reported across settings for each method at the `alpha = 0.05` FDR threshold. 

For easier comparison across settings, we fix the order of the methods first. The ordering is roughly based on the anti-conservative/conservative-ness of each method with covariate-aware methods included as a block.

```{r}
lab_order <- c("Bonferroni", "Benjamini-Hochberg", "q-value", "local FDR",
               "IHW", "Boca-Leek", "FDRreg (theoretical)", "FDRreg (empirical)",
               "ASH", "unadjusted")
```

Similarly, we define a consistent re-naming for each method for easier printing, and also add a column for differentiating covariate-aware methods from the other methods.

```{r}
re_blabel <- function(tab) {
    tsb %>%
        mutate(blabel = fct_recode(f = blabel, "IHW" = "ihw-a05", "ASH" = "ashs",
                                   "Boca-Leek" = "bl-df03", "local FDR" = "lfdr",
                                   "FDRreg (theoretical)" = "scott-theoretical",
                                   "FDRreg (empirical)" = "scott-empirical",
                                   "Benjamini-Hochberg" = "bh",
                                   "Bonferroni" = "bonf", "q-value" = "qvalue"),
               takes_cov = blabel %in%
                   c("IHW", "ASH", "Boca-Leek", "local FDR",
                     "FDRreg (theoretical)", "FDRreg (empirical)")) %>%
        mutate(blabel = factor(blabel, levels = lab_order))
}
```

Results are reported using the following common plotting style.

```{r}
## summarize results in plot
gen_plot <- function(ptitle, tab, co) {
    tab %>%
        ggplot(aes(x = blabel, y = value, color = takes_cov)) +
        geom_boxplot() +
        geom_jitter(alpha = 1/10) +
        scale_y_continuous("", labels = scales::percent) +
        scale_color_brewer(palette = "Paired", guide = FALSE) + 
        xlab("") + 
        theme_bw() +
        theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
        facet_grid(performanceMetric ~ param) +
        geom_hline(aes(yintercept = expected), data = co,
                   color = "blue", alpha = 1/2) +
        ggtitle(ptitle)
}
```


<!-- ######################################################################## -->
## Varying Constant Effect Size
<!-- ######################################################################## -->

<!-- ######################################################################## -->
### Non-Informative Covariate

```{r}
f <- "data-psim/M100/esize_fixed-uniform/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*?)-.*", "\\1", param) %>% as.numeric,
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")),
           param >= 10) %>%
    mutate(param = paste("effect size =", sprintf("%.1f", param / 10)))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.20))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Effect Size, Non-Informative Covariate (100 replications)",
         tsb, cutoffs) 
```

<!-- ######################################################################## -->
### Boca-Leek Style Covariate

```{r}
f <- "data-psim/M100/esize_fixed-bl/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*?)-.*", "\\1", param) %>% as.numeric,
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")),
           param >= 10) %>%
    mutate(param = paste("effect size =", sprintf("%.1f", param / 10)))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.074))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Effect Size, Boca-Leek Style Covariate (100 replications)",
         tsb, cutoffs)
```

<!-- ######################################################################## -->
### Manual Covariate (Step - Less Informative)

```{r}
f <- "data-psim/M100/esize_fixed-bl-step-less/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*?)-.*", "\\1", param) %>% as.numeric,
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")),
           param >= 10) %>%
    mutate(param = paste("effect size =", sprintf("%.1f", param / 10)))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.20))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Effect Size, Manual Covariate (step - less informative) (100 replications)",
         tsb, cutoffs)
```

<!-- ######################################################################## -->
### Manual Covariate (Step - More Informative)

```{r}
f <- "data-psim/M100/esize_fixed-bl-step-more/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*?)-.*", "\\1", param) %>% as.numeric,
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")),
           param >= 10) %>%
    mutate(param = paste("effect size =", sprintf("%.1f", param / 10)))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.20))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Effect Size, Manual Covariate (step - more informative) (100 replications)",
         tsb, cutoffs)
```

<!-- ######################################################################## -->
### Manual Covariate (Cubic)

```{r}
f <- "data-psim/M100/esize_fixed-bl-cubic/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*?)-.*", "\\1", param) %>% as.numeric,
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")),
           param >= 10) %>%
    mutate(param = paste("effect size =", sprintf("%.1f", param / 10)))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.20))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Effect Size, Manual Covariate (cubic) (100 replications)",
         tsb, cutoffs)
```



<!-- ######################################################################## -->
## Varying Null Proportion
<!-- ######################################################################## -->

Need to re-run these simulations with larger test statistic value under the alternative - TPR is too low to be particularly interesting/informative for comparing across methods.

<!-- ######################################################################## -->
### Non-Informative Covariate 

```{r}
f <- "data-psim/M100/pi0-uniform/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove rejections: scaling is off from rest (also dup'd w/ rejprop)
## remove FWER: not informative as a boxplot
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*?)-.*", "\\1", param) %>% as.numeric,
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER"))) %>%
    mutate(param = factor(paste0("null = ", param * 10, "%"),
                          levels = paste0("null = ", seq(10, 100, 10), "%")))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric =
                          factor(c(rep("FPR", 10), rep("rejectprop", 10)),
                                 levels=levels(tsb$performanceMetric)),
                      param = rep(paste0("null = ", seq(10, 100, 10), "%"), 2),
                      expected = c(rep(0.05, 10), seq(.9, 0, -.1)))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Null Proportion, Non-Informative Covariate (100 replications)",
         tsb, cutoffs)
```

<!-- ######################################################################## -->
## Randomly-Sampled UA Effect Size
<!-- ######################################################################## -->

Need to re-run these simulations with lower sampling variance - TPR is too low to be particularly interesting/informative for comparing across methods.

<!-- ######################################################################## -->
### Non-Informative Covariate

```{r}
f <- "data-psim/M100/esize_random_ua-uniform/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*)-.+", "\\1", param),
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.20))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Randomly Sampled UA Effect, Non-Informative Covariate (100 replications)",
         tsb, cutoffs)
```

<!-- ######################################################################## -->
### Boca-Leek Style Covariate

```{r}
f <- "data-psim/M100/esize_random_ua-bl/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*)-.+", "\\1", param),
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.074))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Randomly Sampled UA Effect, Boca-Leek Style Covariate (100 replications)",
         tsb, cutoffs)
```

<!-- ######################################################################## -->
### Manual Covariate (Step - Less Informative)

```{r}
f <- "data-psim/M100/esize_random_ua-bl-step-less/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*)-.+", "\\1", param),
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.20))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Randomly Sampled UA Effect, Manual Covariate (step - less informative) (100 replications)",
         tsb, cutoffs)
```

<!-- ######################################################################## -->
### Manual Covariate (Step - More Informative)

```{r}
f <- "data-psim/M100/esize_random_ua-bl-step-more/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*)-.+", "\\1", param),
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.20))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Randomly Sampled UA Effect, Manual Covariate (step - more informative) (100 replications)",
         tsb, cutoffs)
```

<!-- ######################################################################## -->
### Manual Covariate (Cubic)

```{r}
f <- "data-psim/M100/esize_random_ua-bl-cubic/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*)-.+", "\\1", param),
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.20))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Randomly Sampled UA Effect, Manual Covariate (cubic) (100 replications)",
         tsb, cutoffs)
```


<!-- ######################################################################## -->
## Randomly-Sampled Mean-Shift Effect Size
<!-- ######################################################################## -->

Chi-sq simulations look reasonable, but probably need to re-run the normal and t-test simulations with larger test statistic value under the alternative (possibly 3) - TPR is too low with shift of 1 and 2 to be particularly interesting/informative for comparing across methods.

<!-- ######################################################################## -->
### Non-Informative Covariate

```{r}
f <- "data-psim/M100/esize_random_shift-uniform/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*)-.+", "\\1", param),
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.20))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Randomly Sampled Mean Shifted Effect, Non-Informative Covariate (100 replications)",
         tsb, cutoffs)
```

<!-- ######################################################################## -->
### Boca-Leek Style Covariate

```{r}
f <- "data-psim/M100/esize_random_shift-bl/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*)-.+", "\\1", param),
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.074))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Randomly Sampled Mean Shifted Effect, Boca-Leek Style Covariate (100 replications)",
         tsb, cutoffs)
```

<!-- ######################################################################## -->
### Manual Covariate (Step - Less Informative)

```{r}
f <- "data-psim/M100/esize_random_shift-bl-step-less/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*)-.+", "\\1", param),
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.20))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Randomly Sampled Mean Shifted Effect, Manual Covariate (step - less informative) (100 replications)",
         tsb, cutoffs)
```

<!-- ######################################################################## -->
### Manual Covariate (Step - More Informative)

```{r}
f <- "data-psim/M100/esize_random_shift-bl-step-more/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*)-.+", "\\1", param),
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.20))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Randomly Sampled Mean Shifted Effect, Manual Covariate (step - more informative) (100 replications)",
         tsb, cutoffs)
```

<!-- ######################################################################## -->
### Manual Covariate (Cubic)

```{r}
f <- "data-psim/M100/esize_random_shift-bl-cubic/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*)-.+", "\\1", param),
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.20))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Randomly Sampled Mean Shifted Effect, Manual Covariate (cubic) (100 replications)",
         tsb, cutoffs)
```


<!-- ######################################################################## -->
## Varying Parameter Sampling Distribution
<!-- ######################################################################## -->

Similar as the simulation settings considered above: chi-sq simulations look reasonable, but probably need to re-run the normal and t-test simulations with larger test statistic value under the alternative (possibly 3) - TPR is too low with shift of 1 and 2 to be particularly interesting/informative for comparing across methods.

<!-- ######################################################################## -->
### Non-Informative Covariate

```{r}
f <- "data-psim/M100/altnoise-uniform/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*)-.+", "\\1", param),
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.20))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Sampling Distribution, Non-Informative Covariate (100 replications)",
         tsb, cutoffs)
```

<!-- ######################################################################## -->
### Boca-Leek Style Covariate

```{r}
f <- "data-psim/M100/altnoise-bl/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*)-.+", "\\1", param),
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.074))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Sampling Distribution, Boca-Leek Style Covariate (100 replications)",
         tsb, cutoffs)
```

<!-- ######################################################################## -->
### Manual Covariate (Step - Less Informative)

```{r}
f <- "data-psim/M100/altnoise-bl-step-less/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*)-.+", "\\1", param),
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.20))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Sampling Distribution, Manual Covariate (step - less informative) (100 replications)",
         tsb, cutoffs)
```

<!-- ######################################################################## -->
### Manual Covariate (Step - More Informative)

```{r}
f <- "data-psim/M100/altnoise-bl-step-more/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*)-.+", "\\1", param),
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.20))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Sampling Distribution, Manual Covariate (step - more informative) (100 replications)",
         tsb, cutoffs)
```

<!-- ######################################################################## -->
### Manual Covariate (Cubic)

```{r}
f <- "data-psim/M100/altnoise-bl-cubic/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*)-.+", "\\1", param),
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.20))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Sampling Distribution, Manual Covariate (cubic) (100 replications)",
         tsb, cutoffs)
```

<!-- ######################################################################## -->
## Varying Parameter Sampling Distribution (all null)
<!-- ######################################################################## -->

Similar simulation settings as above, where various sampling distributions are considered, but with `pi0 = 1`, i.e. all null hypotheses.

<!-- ######################################################################## -->
### Non-Informative Covariate

```{r}
f <- "data-psim/M100/allnull-uniform/results-performanceMetrics-alpha05.rds"
tsb <- readRDS(f)

outp <- gsub("data", "plots", dirname(f))
dir.create(outp, showWarnings=FALSE, recursive=TRUE)

## remove non-percentage metric (rejections),
## subset on settings (effect < 1.0 not really informative)
tsb %<>%
    mutate(param = gsub(".*?-.*?_(.*)-.+", "\\1", param),
           performanceMetric =
               factor(performanceMetric,
                      levels = c("rejections", "rejectprop", "TPR",
                                 "TNR", "FPR", "FNR", "FWER"))) %>%
    filter(!(performanceMetric %in% c("rejections", "FWER")))

## relabel methods for easier reading
tsb %<>% re_blabel

## expected thresholds based on simulation setting
cutoffs <- data.frame(performanceMetric = factor(c("FPR", "rejectprop"),
                                                 levels=levels(tsb$performanceMetric)),
                      expected = c(0.05, 0.00))

## summarize results in plot
gen_plot("Direct Parameter Simulation w/ Varying Sampling Dist (all null), Non-Informative Covariate (100 replications)",
         tsb, cutoffs)
```
