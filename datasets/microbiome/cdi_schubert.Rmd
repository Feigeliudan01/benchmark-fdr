---
title: "CDI Schubert"
author: "Claire Duvallet"
date: "9/21/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary

This script will download one processed OTU table from Zenodo, calculate p-values for
OTU abundances in cases vs. controls, and also calculate some covariates like ubiquity,
abundance, and average reads.

## Data

Note: later, we'll be using the following datasets:

- cdi_schubert: 154 healthy, 93 CDI, 89 nonCDI (we can either do CDI vs. healthy or any diarrhea vs. healthy)
- ob_goodrich: 428 healthy, 185 obese
- crc_baxter: 172 healthy, 120 CRC
- ibd_gevers: 16 nonIBD, 146 Crohn's
      - note: the two largest IBD datasets are very uneven (ibd_morgan has 18 healthy, 108 IBD). We could 
  aso use ibd_papa, which has 24 nonIBD and 66 IBD

### CDI Schubert

Let's start with the cdi_schubert dataset first...

```{r}
library(dplyr)
library(ggplot2)

setwd("/Users/claire/github/benchmark-fdr/datasets/microbiome")
source("../../simulation_studies/R/simulation-helpers.R")
source("../R/evaluation-helpers.R")
resdir <- "/Users/claire/github/benchmark-fdr/datasets/microbiome/"
```

```{bash, schubert-download, eval=FALSE}
curl -O "https://zenodo.org/record/840333/files/cdi_schubert_results.tar.gz"
tar -xzvf cdi_schubert_results.tar.gz
```

```{r, results='hide'}
# load in dbOTU table.
otu <- read.table('cdi_schubert_results/RDP/cdi_schubert.otu_table.dbOTU.rdp_assigned')
# and metadata
meta <- read.csv('cdi_schubert_results/cdi_schubert.metadata.txt', sep='\t')
head(otu)
```

```{r, results='hide'}
## Clean up the data

# Keep only samples with DiseaseState metadata
labels <- c("H", "nonCDI", "CDI")

print(dim(meta))
meta <- meta %>% filter(DiseaseState %in% labels)
print(dim(meta))

# Metadata and 16S data
keep_samples <- intersect(colnames(otu), meta$sample_id)
otu <- otu[, keep_samples]
meta <- meta %>% filter(sample_id %in% keep_samples)
print(dim(otu))
print(dim(meta))

# Remove OTUs and samples without enough reads
n_reads_samples <- 1000
n_reads_otus <- 10

remove_shallow_smpls <- function(df, n_reads) {
  # Removes samples with fewer than n_reads from dataframe df.
  # df has OTUs in rows and samples in columns
  return(df[, colSums(df) > n_reads])
  }

remove_shallow_otus <- function(df, n_reads){
  return(df[rowSums(df) > n_reads, ])
  }

print(dim(otu))
otu <- remove_shallow_smpls(otu, n_reads_samples)
otu <- remove_shallow_otus(otu, n_reads_otus)
print(dim(otu))

# Update metadata with new samples
meta <- meta %>% filter(sample_id %in% colnames(otu))

```  

```{r, results='hide'}
## Calculate some covariates

# Mean read depth
reads <- rowMeans(otu)

# Ubiquity
ubi <- rowSums(otu > 0) / length(rownames(otu))

# Mean relative abundance (only considering people who have the OTU present)
abun_otu <- t(t(otu) / rowSums(t(otu)))
mean_abun <- rowSums(abun_otu) / rowSums(abun_otu > 0)

# Make new dataframe to store covariates and eventual p-values
schubert <- data.frame(otu = rownames(otu), mean_abun = mean_abun,
                       ubiquity = ubi, mean_reads = reads)
head(schubert)
```

```{r}
## Calculate p-values between case (CDI or nonCDI) and controls (H)
## Use the relative abundances!

# just do a for loop and ask how to vectorize l8er
case_idx <- meta$DiseaseState %in% c("CDI", "nonCDI")
ctrl_idx <- meta$DiseaseState %in% c("H")
pvals <- c()
ses <- c()
effs <- c()
for (o in rownames(abun_otu)) {
  p <- wilcox.test(abun_otu[o, ctrl_idx], 
                   abun_otu[o, case_idx])$p.value
  pvals <- c(pvals, p)
  
  # standard error
  ses <- c(ses, sd(abun_otu[o,])/sqrt(length(abun_otu[o,])))
  
  # effect (difference of means)
  effs <- c(effs, mean(abun_otu[o, ctrl_idx]) - mean(abun_otu[o, case_idx]))
}
```

```{r}
schubert$pval <- pvals
schubert$SE <- ses
schubert$effect_size <- effs

# Set up ubiquity as the first covariate to test.
# Ranges from 0.001 to 0.4
schubert <- schubert %>% 
              mutate(zscore = effect_size/SE) %>%
              mutate(ind_covariate = log10(ubiquity) )
```

```{r, results='hide', eval = FALSE}
head(schubert)
```

```{r}
# Start with ubiquity as the covariate
# TODO: check independence under the null

# Copied from Keegan's code :)
cutoffs <- c(0.01, 0.025, 0.05, 0.10, 0.20)
resfile <- paste0(resdir, "cdi_schubert_results_",
                  nrow(schubert), ".RData")
t1 <- proc.time()
schu_results <- run_benchmarks(dat=schubert, alphas=cutoffs)
save(schu_results, file=resfile)
message(paste0("Took ", round((proc.time()-t1)[3],1),
               " seconds for ", nrow(schubert), " OTUs"))
```

```{r}
# plot % rejects vs FDR cutoff
schu_results <- schu_results %>% mutate(perc_reject = n_rejects / length(rownames(abun_otu)))
p <- ggplot(schu_results, aes(alpha, perc_reject, color=method)) +
  geom_point() +
  geom_line() +
  xlab("FDR cutoff") + ylab("Fraction of hypotheses rejected") +
  ggtitle("CDI Schubert with log10(Ubiquity) Covariate") +
  theme_classic()
print(p)


```