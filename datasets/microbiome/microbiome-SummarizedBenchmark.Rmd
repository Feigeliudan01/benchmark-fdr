---
title: "Comparison of FDR methods on 16S microbiome data with SummarizedBenchmark"
author: "Claire Duvallet"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_knit$set(root.dir = "/n/irizarryfs01_backed_up/kkorthauer/FDR")
```

### Summary

In the previous document `cdi_schubert.Rmd`, we explored two microbiome datasets for FDR
benchmarking. Here we will make use of the new `SummarizedBenchmark` package to
perform the benchmarking in a standardized way. 

### Workspace setup

```{r, wkspace-setup, results='hide', message=FALSE, warning=FALSE}
# project directory & data/results folders
setwd("/Users/claire/github/benchmark-fdr/datasets/microbiome/")
datdir <- "/Users/claire/github/benchmark-fdr/datasets/microbiome/DATA/"
resdir <- "/Users/claire/github/benchmark-fdr/datasets/microbiome/RESULTS/"
  
# wrangling & plotting tools
library(data.table)
#library(readxl)
library(dplyr)
library(ggplot2)
library(magrittr)
library(R.utils)
library(cowplot)

# benchmark methods
library(IHW)
library(ashr)
library(qvalue)
library(swfdr)
library(fdrtool)
library(FDRreg)

# comparison tools/functions
library(SummarizedBenchmark)  
sourceDirectory("/Users/claire/github/benchmark-fdr/datasets/R/")

## set up parallel backend 
library(BiocParallel)
cores <- as.numeric(Sys.getenv("SLURM_NTASKS"))
multicoreParam <- MulticoreParam(workers = cores)
```

Let's just keep going and hopefully it's okay...

### CDI Schubert - diarrhea dataset

Here we download and analyze the CDI Schubert diarrhea (C. diff and non-C.diff diarrhea) dataset. 
We'll download the processed OTU tables from Zenodo and unzip them in the 
`benchmark-fdr/datasets/microbiome/DATA` folder.

#### Data download

```{bash, schubert-download, eval=FALSE}
cd /Users/claire/github/benchmark-fdr/datasets/microbiome/DATA/
curl -O "https://zenodo.org/record/840333/files/cdi_schubert_results.tar.gz"
tar -xzvf cdi_schubert_results.tar.gz
cd ..
```

Next, we'll read in the unzipped OTU table and metadata files into R.

We'll need to manually define which disease labels are of interest and what metadata
column contains the sample IDs (i.e. the IDs in the OTU table).

```{r, schubert-readdata, eval=TRUE}
otu_path = paste0(datdir, "cdi_schubert_results/RDP/cdi_schubert.otu_table.100.denovo.rdp_assigned")
meta_path = paste0(datdir, "cdi_schubert_results/cdi_schubert.metadata.txt")

# DiseaseState labels to keep
labels <- c("H", "nonCDI", "CDI")

# Metadata column that has sample IDs
col_label <- 'sample_id'

# load OTU table and metadata
otu <- read.table(otu_path)
meta <- read.csv(meta_path, sep='\t')

# Keep only samples with the right DiseaseState metadata
meta <- meta %>% filter(DiseaseState %in% labels)

# Keep only samples with both metadata and 16S data
keep_samples <- intersect(colnames(otu), meta[, 1])
otu <- otu[, keep_samples]
meta <- meta %>% filter(get(col_label) %in% keep_samples)

```

Since we'll be using OTU-wise covariates, we shouldn't need to perform any
filtering/cleaning of the OTUs, apart from removing any that are all zeros. 
(This may happen after removing shallow samples, I think.)
However, let's make sure we get rid of any samples with too few reads. 
We define the minimum number of reads per sample in `sample_reads`. 

After we've removed any shallow samples, we'll convert the OTU table to relative
abundances. I'll add a pseudo-count of 1e-6 to any zero entries, to avoid
problems with taking logs.

```{r, schubert-cleandata, eval=TRUE}
remove_shallow_smpls <- function(df, n_reads) {
  # Removes samples with fewer than n_reads from dataframe df.
  # df has OTUs in rows and samples in columns
  return(df[, colSums(df) > n_reads])
  }

remove_shallow_otus <- function(df, n_reads){
  return(df[rowSums(df) > n_reads, ])
  }

## Remove OTUs with fewer than 10 reads
otu <- remove_shallow_otus(otu, 10)

# Remove samples with fewer than sample_Reads reads
sample_reads <- 100
otu <- remove_shallow_smpls(otu, sample_reads)

# Update metadata with new samples
meta <- meta %>% filter(get(col_label) %in% colnames(otu))

# Remove empty OTUs
otu <- otu[rowSums(otu) > 0, ]

# Convert to relative abundance
abun_otu <- t(t(otu) / rowSums(t(otu)))

# Add pseudo counts
#minabun <- min(abun_otu[abun_otu > 0]) # Use this to figure out what pseudo-count value to add
zeroabun <- 0
abun_otu <- abun_otu + zeroabun

```

Next, we need to calculate the pvalues, effect size, and standard error for each OTU.
Here, we'll compare diarrhea vs. healthy. Diarrhea will include both CDI and nonCDI
patients. We'll put these results into a dataframe, and label the columns with the 
standardized names for downstream use (`pval`, `SE`, `effect_size`, `test_statistic`).
The test statistic is the one returned by `wilcox.test()`.

Note that the effect here is calculated as logfold change of mean abundance in controls
relative to cases (i.e. `log(mean_abun[controls]/mean_abun[cases])`)

While we're at it, we'll also calculate the mean abundance and ubiquity (detection rate)
of each OTU. Later, we can assign their values to a new column called `ind_covariate` 
for use in downstream steps.

```{r, schubert-pvals, eval=TRUE}
resfile <- paste0(resdir, "schubert_results_",
                  nrow(abun_otu), "_OTUs.RData")
if (!file.exists(resfile)){
  # Get case and control indices
  case_idx <- meta$DiseaseState %in% c("CDI", "nonCDI")
  ctrl_idx <- meta$DiseaseState %in% c("H")
  
  # Calculate pvalues, effects, and stderr
  pvals <- c()
  teststats <- c()
  ses <- c()
  effs <- c()
  mean_abuns <- c()
  mean_abuns_present <- c()
  ubis <- c()
  
  casedf <- abun_otu[, case_idx]
  ctrldf <- abun_otu[, ctrl_idx]
  for (o in rownames(abun_otu)) {
    # wilcoxon p value  
    w <- wilcox.test(casedf[o, ], 
                     ctrldf[o, ])
    p <- w$p.value
    teststat <- w$statistic
    
    pvals <- c(pvals, p)
    teststats <- c(teststats, teststat)
    
    # standard error of the OTU abundance, across all samples
    ses <- c(ses, 
             sd(abun_otu[o, ])/sqrt(length(abun_otu[o, ]))
             )
    
    # mean OTU abundance across all samples (after removing pseudo-count)
    mean_abuns <- c(mean_abuns, 
                    mean(abun_otu[o, ] - zeroabun)
                    )
    
    # mean OTU abundance across only samples with the OTU present
    mean_abuns_present <- c(mean_abuns_present, 
                            sum(abun_otu[o, ] - zeroabun) / sum(abun_otu[o, ] > zeroabun)
                            )
    
    # ubiquity of OTU across all samples
    ubis <- c(ubis, 
              sum(abun_otu[o, ] > zeroabun) / length(abun_otu[o, ])
              )
    
    # effect (logfold difference)
    effs <- c(effs, 
              log(mean(abun_otu[o, ctrl_idx])/mean(abun_otu[o, case_idx]))
              )
  
  }
  
  res <- data.frame(otu = rownames(abun_otu), 
                    pval = pvals, wilcox_teststat = teststats,
                    SE = ses, effect_size = effs, 
                    mean_abun = mean_abuns, mean_abun_present = mean_abuns_present,
                    ubiquity = ubis)
  res <- res %>%
    mutate(test_statistic = qnorm(exp(log(pval)-log(2)), lower.tail=FALSE) * sign(effect_size))

  save(res, file=resfile)
  
}else{
  load(resfile)
}
head(res)
```

Finally, let's try to add phylogeny as covariates. Here we'll have columns for each separate taxonomic level.

```{r, schubert-addphylo, eval=TRUE}
res <- res %>% separate(otu, 
                        c("kingdom", "phylum", "class", "order", "family", "genus", "species", "denovo"), 
                        sep=";", remove = FALSE)

```

#### Check Covariate Diagnostics

Here we look to see if the covariates do indeed look informative.

##### Ubiquity

```{r, schubert-ubi, fig.width=10, fig.height=3.5, eval=TRUE}
strat_hist(res, pvalue="pval", covariate="ubiquity", maxy=20, binwidth=0.05)
rank_scatter(res, pvalue="pval", covariate="ubiquity")
```

##### Mean abundance (across non-zero samples)

```{r, schubert-abun, fig.width=10, fig.height=3.5, eval=TRUE}
strat_hist(res, pvalue="pval", covariate="mean_abun_present", maxy=8, binwidth=0.05)
rank_scatter(res, pvalue="pval", covariate="mean_abun_present")
```

##### Phylogeny

Let's look at phylum-level stratification first. A priori, I *might* expect
Proteobacteria to be enriched for low p-values? But I don't know if that's
super legit, and Eric doesn't seem to think that phylogeny will be informative at all...

```{r, schubert-phylo, fig.width=10, fig.height=6, eval=FALSE}
#strat_hist(res, pvalue="pval", covariate="mean_abun_present", maxy=20)
#rank_scatter(res, pvalue="pval", covariate="ubiquity")
ggplot(res, aes(x=pval)) + geom_histogram() + facet_wrap(~phylum, scales = "free")
```

Let's use `ubiquity` as our `ind_covariate`

```{r, schubert-labelcovariate, eval=TRUE}
res <- res %>% mutate(ind_covariate = ubiquity)
```

Also (hacky), remove test_statistic so that ash doesn't run.

```{r, schubert-rm-ash, eval=TRUE}
res <- res %>% 
  mutate(test_statistic = NA) %>%
  mutate(effect_size = NA)
```

#### Set up BenchDesign object 

First, we'll create an object of `BenchDesign` class to hold the data and 
add the benchmark methods to the `BenchDesign` object. We'll do this for a few
values of nmids, to explore its effects on the outputs.

Then, we'll construct the `SummarizedBenchmark` object, which will run
the functions specified in each method (these are actually sourced in from the
helper scripts). 

```{r, schubert-sb, results="hide", message=FALSE, eval=TRUE}
for (nmids in c(5, 50, 150)){
  print(nmids)
  bd <- initializeBenchDesign(nmids=nmids)
  
  resfile <- paste0(resdir, "schubert_summarizedBenchmark_",
                    nrow(res), "_nmids_", nmids, ".RData")
  duration <- NA
  if (!file.exists(resfile)){
    t1 <- proc.time()
    sb <- bd %>% buildBench(data=res, ftCols = "ind_covariate")
                            #parallel=TRUE, BPPARAM=multicoreParam)
    metadata(sb)$data_download_link <- "https://zenodo.org/record/840333/files/cdi_schubert_results.tar.gz"
    save(sb, file=resfile)
    duration <- round((proc.time()-t1)[3]/60,1)
  }else{
    load(resfile)
  }
}

```

Just for future runs, where I load the file instead of re-calculating it, here are the errors I got:

```
[1] 5
Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !NaNs produced<simpleError in if (sum(D^2) > qchisq(0.9, nmids - 2 - df)) {    warning(paste0("f(z) misfit = ", round(D, 1), ".  Rerun with increased df."))}: missing value where TRUE/FALSE needed>

[1] 50
Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !f(z) misfit = -21.6.  Rerun with increased df.f(z) misfit = -22.5.  Rerun with increased df.f(z) misfit = -10.  Rerun with increased df.f(z) misfit = 32.  Rerun with increased df.f(z) misfit = 14.  Rerun with increased df.f(z) misfit = -32.2.  Rerun with increased df.f(z) misfit = 6.9.  Rerun with increased df.f(z) misfit = -1.3.  Rerun with increased df.f(z) misfit = 0.7.  Rerun with increased df.f(z) misfit = 3.4.  Rerun with increased df.f(z) misfit = 5.5.  Rerun with increased df.f(z) misfit = 0.1.  Rerun with increased df.f(z) misfit = -0.9.  Rerun with increased df.f(z) misfit = -1.5.  Rerun with increased df.f(z) misfit = -1.9.  Rerun with increased df.f(z) misfit = -0.8.  Rerun with increased df.f(z) misfit = -2.4.  Rerun with increased df.f(z) misfit = 1.7.  Rerun with increased df.f(z) misfit = 0.5.  Rerun with increased df.f(z) misfit = 2.  Rerun with increased df.f(z) misfit = 0.1.  Rerun with increased df.f(z) misfit = 1.4.  Rerun with increased df.f(z) misfit = 0.2.  Reru... <truncated>

[1] 150
Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !f(z) misfit = 83.4.  Rerun with increased df.f(z) misfit = -8.5.  Rerun with increased df.f(z) misfit = -13.6.  Rerun with increased df.f(z) misfit = -17.1.  Rerun with increased df.f(z) misfit = -6.4.  Rerun with increased df.f(z) misfit = -19.5.  Rerun with increased df.f(z) misfit = -17.8.  Rerun with increased df.f(z) misfit = -3.  Rerun with increased df.f(z) misfit = -21.1.  Rerun with increased df.f(z) misfit = 28.6.  Rerun with increased df.f(z) misfit = -26.  Rerun with increased df.f(z) misfit = 59.3.  Rerun with increased df.f(z) misfit = -17.1.  Rerun with increased df.f(z) misfit = 12.1.  Rerun with increased df.f(z) misfit = -28.3.  Rerun with increased df.f(z) misfit = -27.6.  Rerun with increased df.f(z) misfit = 83.7.  Rerun with increased df.f(z) misfit = -10.5.  Rerun with increased df.f(z) misfit = -22.6.  Rerun with increased df.f(z) misfit = -23.3.  Rerun with increased df.f(z) misfit = 37.3.  Rerun with increased df.f(z) misfit = -18.  Rerun with increased df.f(z... <truncated>>
```

```{r, echo=FALSE, eval=FALSE}
if(!is.na(duration)){
  message("This step took ", duration, " minutes for ", nrow(res), " OTUs using ",
        cores, " cores.")
}
```

Next, we'll add the default performance metric for q-value assays. First, we have
to rename the assay to 'qvalue'.

```{r, schubert-metrics, eval=TRUE}
# rename assay to qvalue
assayNames(sb) <- "qvalue"
sb <- addDefaultMetrics(sb)
```

Now, we'll plot the results.

Debugging note: to look at the matrix of qvalues, run `assays(sb)[["qvalue"]]`

```{r, schubert-plot, width=15, height=15, eval=TRUE}

# plot nrejects by method overall and stratified by covariate
rejections_scatter( sb, as_fraction=FALSE, supplementary=FALSE)
rejection_scatter_bins(sb, covariate="ind_covariate", supplementary=FALSE)
plotFDRMethodsOverlap(sb, alpha=0.1, supplementary=FALSE, order.by="freq", nsets=100 )

```

Hm, now the code runs. However, there are clearly still some issues:
- ashs rejects all hypotheses (all q-values are essentially 0).
- lfdr and scott-empirical are all NaN (I think this is likely related to the df error)

```{r, schubert-genus-covariates}
methods <- c( "lfdr", "ihw-a10", "bl-df03", "qvalue", "bh", "bonf" )
plotCovariateBoxplots( sb, alpha=0.1, nsets=6, methods=methods)
```


```{r, eval=FALSE}
assays(sb)[["qvalue"]]["ashs"] %>% max()
sum(is.na(assays(sb)[["qvalue"]]["lfdr"]))
sum(is.na(assays(sb)[["qvalue"]]["scott-empirical"]))
sum(is.na(assays(sb)[["qvalue"]]["scott-theoretical"]))
```

Plotting methods are giving errors for some reason. Let's try to use Alejandro's code instead.


And we'll clean up the workspace before moving on to the next dataset.

```{r, schubert-cleanup, eval=FALSE}
rm(res)
rm(bd)
rm(sb)
rm(pf)
```

### OB Goodrich - obesity dataset

Let's repeat these analyses for the OB Goodrich dataset.

Here we download and analyze the CDI Schubert diarrhea (C. diff and non-C.diff diarrhea) dataset. 
We'll download the processed OTU tables from Zenodo and unzip them in the 
`benchmark-fdr/datasets/microbiome/DATA` folder.

#### Data download

```{bash, goodrich-download, eval=FALSE}
cd /Users/claire/github/benchmark-fdr/datasets/microbiome/DATA/
curl -O "https://zenodo.org/record/840333/files/ob_goodrich_results.tar.gz"
tar -xzvf ob_goodrich_results.tar.gz
cd ..
```

Next, we'll read in the unzipped OTU table and metadata files into R.

We'll need to manually define which disease labels are of interest and what metadata
column contains the sample IDs (i.e. the IDs in the OTU table).

```{r, goodrich-readdata}
otu_path = paste0(datdir, "ob_goodrich_results/RDP/ob_goodrich.otu_table.100.denovo.rdp_assigned")
meta_path = paste0(datdir, "ob_goodrich_results/ob_goodrich.metadata.txt")

# DiseaseState labels to keep
labels <- c("H", "OB")

# Metadata column that has sample IDs
col_label <- "X"

# load OTU table and metadata
otu <- read.table(otu_path)
meta <- read.csv(meta_path, sep='\t')

# Keep only samples with the right DiseaseState metadata
meta <- meta %>% filter(DiseaseState %in% labels)

# Keep only samples with both metadata and 16S data
keep_samples <- intersect(colnames(otu), meta[, 1])
otu <- otu[, keep_samples]
meta <- meta %>% filter(get(col_label) %in% keep_samples)

```


Since we'll be using OTU-wise covariates, we shouldn't need to perform any
filtering/cleaning of the OTUs, apart from removing any that are all zeros. 
(This may happen after removing shallow samples, I think.)
However, let's make sure we get rid of any samples with too few reads. 
We define the minimum number of reads per sample in `sample_reads`. 

After we've removed any shallow samples, we'll convert the OTU table to relative
abundances. I'll add a pseudo-count of 1e-6 to any zero entries, to avoid
problems with taking logs.

```{r, goodrich-cleandata}
remove_shallow_smpls <- function(df, n_reads) {
  # Removes samples with fewer than n_reads from dataframe df.
  # df has OTUs in rows and samples in columns
  return(df[, colSums(df) > n_reads])
}

remove_shallow_otus <- function(df, n_reads){
  return(df[rowSums(df) > n_reads, ])
}

remove_rare_otus <- function(df, perc_samples){
  return(df[rowSums(df > 0) / dim(df)[2] > perc_samples, ])
}

## Remove OTUs with fewer than 10 reads
otu <- remove_shallow_otus(otu, 10)
print(dim(otu))
## Remove OTUs in fewer than 1% of samples
otu <- remove_rare_otus(otu, 0.01)
print(dim(otu))

# Remove samples with fewer than sample_reads reads
sample_reads <- 100
otu <- remove_shallow_smpls(otu, sample_reads)

# Update metadata with new samples
meta <- meta %>% filter(get(col_label) %in% colnames(otu))

# Remove empty OTUs
otu <- otu[rowSums(otu) > 0, ]

# Convert to relative abundance
abun_otu <- t(t(otu) / rowSums(t(otu)))

# Add pseudo counts
#minabun <- min(abun_otu[abun_otu > 0]) # Use this to figure out what pseudo-count value to add
zeroabun <- 0
abun_otu <- abun_otu + zeroabun

```

Next, we need to calculate the pvalues, effect size, and standard error for each OTU.
Here, we'll compare lean vs. obese. We'll put these results into a dataframe, and label 
the columns with the standardized names for downstream use 
(`pval`, `SE`, `effect_size`, `test_statistic`). The test statistic is the one returned 
by `wilcox.test()`.

Note that the effect here is calculated as logfold change of mean abundance in controls
relative to cases (i.e. `log(mean_abun[controls]/mean_abun[cases])`)

While we're at it, we'll also calculate the mean abundance and ubiquity (detection rate)
of each OTU. Later, we can assign their values to a new column called `ind_covariate` 
for use in downstream steps.

Note that OB Goodrich has ~70,000 OTUs, so this is going to take a while. There are probably
faster ways to write this, but I don't know them.

```{r}
dim(abun_otu)
```

```{r, goodrich-pvals}
resfile <- paste0(resdir, "goodrich_results_",
                  nrow(abun_otu), "_OTUs.RData")
if (!file.exists(resfile)){
  # Get case and control indices
  case_idx <- meta$DiseaseState %in% c("OB")
  ctrl_idx <- meta$DiseaseState %in% c("H")
  
  # Calculate pvalues, effects, and stderr
  pvals <- c()
  teststats <- c()
  ses <- c()
  effs <- c()
  mean_abuns <- c()
  mean_abuns_present <- c()
  ubis <- c()
  
  casedf <- abun_otu[, case_idx]
  ctrldf <- abun_otu[, ctrl_idx]
  for (o in rownames(abun_otu)) {
    # wilcoxon p value  
    w <- wilcox.test(casedf[o, ], 
                     ctrldf[o, ])
    p <- w$p.value
    teststat <- w$statistic
    
    pvals <- c(pvals, p)
    teststats <- c(teststats, teststat)
    
    # standard error of the OTU abundance, across all samples
    ses <- c(ses, 
             sd(abun_otu[o, ])/sqrt(length(abun_otu[o, ]))
             )
    
    # mean OTU abundance across all samples (after removing pseudo-count)
    mean_abuns <- c(mean_abuns, 
                    mean(abun_otu[o, ] - zeroabun)
                    )
    
    # mean OTU abundance across only samples with the OTU present
    mean_abuns_present <- c(mean_abuns_present, 
                            sum(abun_otu[o, ] - zeroabun) / sum(abun_otu[o, ] > zeroabun)
                            )
    
    # ubiquity of OTU across all samples
    ubis <- c(ubis, 
              sum(abun_otu[o, ] > zeroabun) / length(abun_otu[o, ])
              )
    
    # effect (logfold difference)
    effs <- c(effs, 
              log(mean(abun_otu[o, ctrl_idx])/mean(abun_otu[o, case_idx]))
              )
  
  }
  
  res <- data.frame(otu = rownames(abun_otu), 
                    pval = pvals, test_statistic = teststats,
                    SE = ses, effect_size = effs, 
                    mean_abun = mean_abuns, mean_abun_present = mean_abuns_present,
                    ubiquity = ubis)
  res <- res %>%
    mutate(test_statistic = qnorm(exp(log(pval)-log(2)), lower.tail=FALSE) * sign(effect_size))
  
  save(res, file=resfile)
  
}else{
  load(resfile)
}
head(res)
```


Finally, let's try to add phylogeny as covariates. Here we'll have columns for each separate taxonomic level.

```{r, goodrich-addphylo}
res <- res %>% separate(otu, 
                        c("kingdom", "phylum", "class", "order", "family", "genus", "species", "denovo"), 
                        sep=";", remove = FALSE)

```

#### Check Covariate Diagnostics

Here we look to see if the covariates do indeed look informative.

##### Ubiquity

```{r, goodrich-ubi, fig.width=10, fig.height=3.5}
strat_hist(res, pvalue="pval", covariate="ubiquity", maxy=5, binwidth=0.05, numQ=4)
rank_scatter(res, pvalue="pval", covariate="ubiquity")
```

##### Mean abundance (across non-zero samples)

```{r, goodrich-abun, fig.width=10, fig.height=3.5}
strat_hist(res, pvalue="pval", covariate="mean_abun_present", maxy=5, binwidth=0.05, numQ=4)
rank_scatter(res, pvalue="pval", covariate="mean_abun_present")
```

##### Phylogeny

Let's look at phylum-level stratification first. A priori, I *might* expect
Proteobacteria to be enriched for low p-values? But I don't know if that's
super legit, and Eric doesn't seem to think that phylogeny will be informative at all...

```{r, goodrich-phylo, fig.width=10, fig.height=6, eval=FALSE}
#strat_hist(res, pvalue="pval", covariate="mean_abun_present", maxy=20)
#rank_scatter(res, pvalue="pval", covariate="ubiquity")
ggplot(res, aes(x=pval)) + geom_histogram() + facet_wrap(~phylum, scales = "free")
```

Let's use `ubiquity` as our `ind_covariate`

```{r, goodrich-labelcovariate}
res <- res %>% mutate(ind_covariate = ubiquity)
```

Also (hacky), remove test_statistic so that ash doesn't run.

```{r, goodrich-rm-ash}
res <- res %>% 
  mutate(test_statistic = NA) %>%
  mutate(effect_size = NA)
```

#### Set up BenchDesign object 

First, we'll create an object of `BenchDesign` class to hold the data and 
add the benchmark methods to the `BenchDesign` object.

Next, we'll construct the `SummarizedBenchmark` object, which will run
the functions specified in each method (these are actually sourced in from the
helper scripts). 

```{r, goodrich-sb, results="hide", message=FALSE, eval=TRUE}
for (nmids in c(5, 50, 150)){
  print(nmids)
  bd <- initializeBenchDesign(nmids=nmids)
  
  resfile <- paste0(resdir, "goodrich_summarizedBenchmark_",
                    nrow(res), "_nmids_", nmids, ".RData")
  duration <- NA
  if (!file.exists(resfile)){
    t1 <- proc.time()
    sb <- bd %>% buildBench(data=res, ftCols = "ind_covariate")
                            #parallel=TRUE, BPPARAM=multicoreParam)
    metadata(sb)$data_download_link <- "https://zenodo.org/record/840333/files/cdi_schubert_results.tar.gz"
    save(sb, file=resfile)
    duration <- round((proc.time()-t1)[3]/60,1)
  }else{
    load(resfile)
  }
}
```

The outputs of this call are:

```
[1] 5
Squarem-1
Objective fn: 715262
Objective fn: 104929  Extrapolation: 0  Steplength: 1
Objective fn: 95918.9  Extrapolation: 1  Steplength: 2.44204
Objective fn: 95614.3  Extrapolation: 1  Steplength: 2.72536
Objective fn: 95583.3  Extrapolation: 1  Steplength: 2.8568
Objective fn: 95578.1  Extrapolation: 1  Steplength: 3.54373
Objective fn: 95576.3  Extrapolation: 1  Steplength: 3.52004
Objective fn: 95575.4  Extrapolation: 1  Steplength: 4
.
.
.
NaNs produced<simpleError in if (sum(D^2) > qchisq(0.9, nmids - 2 - df)) {    warning(paste0("f(z) misfit = ", round(D, 1), ".  Rerun with increased df."))}: missing value where TRUE/FALSE needed>

[1] 50
Squarem-1
Objective fn: 715262
Objective fn: 104929  Extrapolation: 0  Steplength: 1
Objective fn: 95918.9  Extrapolation: 1  Steplength: 2.44204
Objective fn: 95614.3  Extrapolation: 1  Steplength: 2.72536
Objective fn: 95583.3  Extrapolation: 1  Steplength: 2.8568
Objective fn: 95578.1  Extrapolation: 1  Steplength: 3.54373
Objective fn: 95576.3  Extrapolation: 1  Steplength: 3.52004
Objective fn: 95575.4  Extrapolation: 1  Steplength: 4
Objective fn: 95574.9  Extrapolation: 1  Steplength: 3.36769
Objective fn: 95574.5  Extrapolation: 1  Steplength: 5.09115
Objective fn: 95574.3  Extrapolation: 1  Steplength: 2.52066
Objective fn: 95574  Extrapolation: 1  Steplength: 13.3966
.
.
.
Objective fn: 95572.9  Extrapolation: 1  Steplength: 1.61619
f(z) misfit = -8.7.  Rerun with increased df.f(z) misfit = 33.3.  Rerun with increased df.f(z) misfit = -9.4.  Rerun with increased df.f(z) misfit = -37.3.  Rerun with increased df.f(z) misfit = 49.2.  Rerun with increased df.f(z) misfit = -36.9.  Rerun with increased df.f(z) misfit = 10.6.  Rerun with increased df.f(z) misfit = 13.3.  Rerun with increased df.f(z) misfit = -18.3.  Rerun with increased df.f(z) misfit = 20.1.  Rerun with increased df.f(z) misfit = -21.6.  Rerun with increased df.f(z) misfit = 23.4.  Rerun with increased df.f(z) misfit = -27.  Rerun with increased df.f(z) misfit = 17.2.  Rerun with increased df.f(z) misfit = -8.2.  Rerun with increased df.f(z) misfit = 4.3.  Rerun with increased df.f(z) misfit = 3.4.  Rerun with increased df.f(z) misfit = 2.4.  Rerun with increased df.f(z) misfit = -2.3.  Rerun with increased df.f(z) misfit = -10.7.  Rerun with increased df.f(z) misfit = -2.4.  Rerun with increased df.f(z) misfit = 15.4.  Rerun with increased df.f(z) misf... <truncated>

[1] 150
Squarem-1
Objective fn: 715262
Objective fn: 104929  Extrapolation: 0  Steplength: 1
Objective fn: 95918.9  Extrapolation: 1  Steplength: 2.44204
Objective fn: 95614.3  Extrapolation: 1  Steplength: 2.72536
Objective fn: 95583.3  Extrapolation: 1  Steplength: 2.8568
Objective fn: 95578.1  Extrapolation: 1  Steplength: 3.54373
Objective fn: 95576.3  Extrapolation: 1  Steplength: 3.52004
Objective fn: 95575.4  Extrapolation: 1  Steplength: 4
Objective fn: 95574.9  Extrapolation: 1  Steplength: 3.36769
Objective fn: 95574.5  Extrapolation: 1  Steplength: 5.09115
Objective fn: 95574.3  Extrapolation: 1  Steplength: 2.52066
Objective fn: 95574  Extrapolation: 1  Steplength: 13.3966
.
.
.
f(z) misfit = -20.2.  Rerun with increased df.f(z) misfit = 13.6.  Rerun with increased df.f(z) misfit = -3.8.  Rerun with increased df.f(z) misfit = 9.  Rerun with increased df.f(z) misfit = -21.2.  Rerun with increased df.f(z) misfit = 28.8.  Rerun with increased df.f(z) misfit = 41.9.  Rerun with increased df.f(z) misfit = -13.2.  Rerun with increased df.f(z) misfit = -12.8.  Rerun with increased df.f(z) misfit = 15.2.  Rerun with increased df.f(z) misfit = -18.1.  Rerun with increased df.f(z) misfit = -19.1.  Rerun with increased df.f(z) misfit = -18.6.  Rerun with increased df.f(z) misfit = -26.  Rerun with increased df.f(z) misfit = -7.2.  Rerun with increased df.f(z) misfit = 67.1.  Rerun with increased df.f(z) misfit = 27.2.  Rerun with increased df.f(z) misfit = -16.4.  Rerun with increased df.f(z) misfit = -21.1.  Rerun with increased df.f(z) misfit = -25.9.  Rerun with increased df.f(z) misfit = -7.5.  Rerun with increased df.f(z) misfit = 10.1.  Rerun with increased df.f(z)... <truncated>
```

```{r, echo=FALSE, eval=FALSE}
if(!is.na(duration)){
  message("This step took ", duration, " minutes for ", nrow(res), " OTUs using ",
        cores, " cores.")
}
```

Next, we'll add the default performance metric for q-value assays. First, we have
to rename the assay to 'qvalue'.

```{r, goodrich-metrics, eval=TRUE}
# rename assay to qvalue
assayNames(sb) <- "qvalue"
sb <- addDefaultMetrics(sb)
```

Now, we'll plot the results.

Debugging note: to look at the matrix of qvalues, run `assays(sb)[["qvalue"]]`

```{r, goodrich-plot, width=15, height=15, eval=TRUE}
# plot nrejects by method overall and stratified by covariate
rejections_scatter( sb, as_fraction=FALSE, supplementary=FALSE)
rejection_scatter_bins(sb, covariate="ind_covariate", supplementary=FALSE)
#plotFDRMethodsOverlap(sb, alpha=0.1, supplementary=FALSE )
```

There is no upsetR plot for OB Goodrich for now because only lfdr is returning any rejections.

```{r, eval=FALSE}
assays(sb)[["qvalue"]][, "ashs"] %>% max()
sum(is.na(assays(sb)[["qvalue"]][, "scott-empirical"]))
sum(is.na(assays(sb)[["qvalue"]][, "lfdr"]))

```


## Goodrich, genus-level analysis

```{r, goodrich-genus}
## Collapse baxter OTU table to genus level

# Add column with otu names
genus_df <- as.data.frame(abun_otu)
genus_df$otu <- rownames(genus_df)
# Melt into tidy format
genus_df <- genus_df %>% melt(id.var="otu", variable.name="sample", value.name="abun")
# Split into separate taxonomic levels
genus_df <- genus_df %>% 
  separate(otu, c("kingdom", "phylum", "class", "order", "family", "genus", "species", "denovo"), 
           sep=";", remove = FALSE)

# Get rid of unannoated genera, and sum abundances for genera
genus_df <- genus_df %>% 
  filter(genus != "g__") %>% 
  group_by(genus, sample) %>% 
  summarise(total_abun = sum(abun)) 

# Convert back to longform
genus_df <- genus_df %>% spread(genus, total_abun) %>% as.data.frame
rownames(genus_df) <- genus_df$sample
genus_df <- genus_df %>% select(-sample) %>% t

# And re-order columns to match metadata
genus_df <- genus_df[, match(meta$X, colnames(genus_df))]
```


```{r, goodrich-genus-pvals}
resfile <- paste0(resdir, "goodrich_results_",
                  nrow(genus_df), "_genera.RData")
if (!file.exists(resfile)){
  # Get case and control indices
  case_idx <- meta$DiseaseState %in% c("OB")
  ctrl_idx <- meta$DiseaseState %in% c("H")
  
  # Calculate pvalues, effects, and stderr
  pvals <- c()
  teststats <- c()
  ses <- c()
  effs <- c()
  mean_abuns <- c()
  mean_abuns_present <- c()
  ubis <- c()
  
  casedf <- genus_df[, case_idx]
  ctrldf <- genus_df[, ctrl_idx]
  for (o in rownames(genus_df)) {
    # wilcoxon p value  
    w <- wilcox.test(casedf[o, ], 
                     ctrldf[o, ])
    p <- w$p.value
    teststat <- w$statistic
    
    pvals <- c(pvals, p)
    teststats <- c(teststats, teststat)
    
    # standard error of the OTU abundance, across all samples
    ses <- c(ses, 
             sd(genus_df[o, ])/sqrt(length(genus_df[o, ]))
             )
    
    # mean OTU abundance across all samples (after removing pseudo-count)
    mean_abuns <- c(mean_abuns, 
                    mean(genus_df[o, ] - zeroabun)
                    )
    
    # mean OTU abundance across only samples with the OTU present
    mean_abuns_present <- c(mean_abuns_present, 
                            sum(genus_df[o, ] - zeroabun) / sum(genus_df[o, ] > zeroabun)
                            )
    
    # ubiquity of OTU across all samples
    ubis <- c(ubis, 
              sum(genus_df[o, ] > zeroabun) / length(genus_df[o, ])
              )
    
    # effect (logfold difference)
    effs <- c(effs, 
              log(mean(genus_df[o, ctrl_idx])/mean(genus_df[o, case_idx]))
              )
  
  }
  
  res <- data.frame(otu = rownames(genus_df), 
                    pval = pvals, test_statistic = teststats,
                    SE = ses, effect_size = effs, 
                    mean_abun = mean_abuns, mean_abun_present = mean_abuns_present,
                    ubiquity = ubis)
  res <- res %>%
    mutate(test_statistic = qnorm(exp(log(pval)-log(2)), lower.tail=FALSE) * sign(effect_size))
  
  save(res, file=resfile)
  
}else{
  load(resfile)
}
```

#### Check Covariate Diagnostics

Here we look to see if the covariates do indeed look informative.

##### Ubiquity

```{r, goodrich-genus-ubi, fig.width=10, fig.height=3.5}
strat_hist(res, pvalue="pval", covariate="ubiquity", maxy=10, binwidth=0.05)
rank_scatter(res, pvalue="pval", covariate="ubiquity")
```

Something weird is happening with p=0.20 and p=0.40 - maybe this has something to do with ties?
Either way, ubiquity looks to be a bit informative (from the scatter plot, but not really the histograms...)

##### Mean abundance (across non-zero samples)

```{r, goodrich-genus-abun, fig.width=10, fig.height=3.5}
strat_hist(res, pvalue="pval", covariate="mean_abun_present", maxy=8, binwidth=0.05)
rank_scatter(res, pvalue="pval", covariate="mean_abun_present")
```

Let's use `ubiquity` as our `ind_covariate`

```{r, goodrich-genus-labelcovariate}
res <- res %>% mutate(ind_covariate = ubiquity)
```

Also (hacky), remove test_statistic so that ash doesn't run.

```{r, goodrich-genus-rm-ash}
res <- res %>% 
  mutate(test_statistic = NA) %>%
  mutate(effect_size = NA)
```

#### Set up BenchDesign object 

First, we'll create an object of `BenchDesign` class to hold the data and 
add the benchmark methods to the `BenchDesign` object. We'll do this for a few
values of nmids, to explore its effects on the outputs.

Then, we'll construct the `SummarizedBenchmark` object, which will run
the functions specified in each method (these are actually sourced in from the
helper scripts). 

```{r, goodrich-genus-sb, results="hide", message=FALSE, eval=TRUE}
for (nmids in c(5, 50, 150)){
  print(nmids)
  bd <- initializeBenchDesign(nmids=nmids)
  
  resfile <- paste0(resdir, "goodrich_summarizedBenchmark_",
                    nrow(res), "_genera_nmids_", nmids, ".RData")
  duration <- NA
  if (!file.exists(resfile)){
    t1 <- proc.time()
    sb <- bd %>% buildBench(data=res, ftCols = "ind_covariate")
                            #parallel=TRUE, BPPARAM=multicoreParam)
    metadata(sb)$data_download_link <- "https://zenodo.org/record/840333/files/crc_baxter_results.tar.gz"
    save(sb, file=resfile)
    duration <- round((proc.time()-t1)[3]/60,1)
  }else{
    load(resfile)
  }
}

```


Next, we'll add the default performance metric for q-value assays. First, we have
to rename the assay to 'qvalue'.

```{r, goodrich-genus-metrics, eval=TRUE}
# rename assay to qvalue
assayNames(sb) <- "qvalue"
sb <- addDefaultMetrics(sb)
```

Now, we'll plot the results.

Debugging note: to look at the matrix of qvalues, run `assays(sb)[["qvalue"]]`

```{r, goodrich-genus-plot, width=15, height=15, eval=TRUE}
# plot nrejects by method overall and stratified by covariate
rejections_scatter( sb, as_fraction=FALSE, supplementary=FALSE)
rejection_scatter_bins(sb, covariate="ind_covariate", supplementary=FALSE)
#plotFDRMethodsOverlap(sb, alpha=0.1, supplementary=FALSE, order.by="freq", nsets=100 )
```

Note: Benjamini-Hochberg (bh) overlaps exactly with the IHW results.

```{r, goodrich-genus-overlap}
plotFDRMethodsOverlap(sb, alpha=0.1, supplementary=FALSE, order.by="freq", nsets=100 )
```
```{r, goodrich-genus-covariates}
methods <- c( "lfdr", "ihw-a10", "bl-df03", "qvalue", "bh", "bonf" )
plotCovariateBoxplots( sb, alpha=0.1, nsets=6, methods=methods)
```

### CRC Baxter - colorectal cancer

Next, I'll look at the CRC Baxter dataset. For colorectal cancer, we do expect some amount of truly differentially abundant OTUs, but not as many as in diarrhea. This dataset will hopefully provide an intermediate non-extreme case study.
We'll download the processed OTU tables from Zenodo and unzip them in the 
`benchmark-fdr/datasets/microbiome/DATA` folder.

#### Data download

```{bash, baxter-download, eval=FALSE}
cd /Users/claire/github/benchmark-fdr/datasets/microbiome/DATA/
curl -O "https://zenodo.org/record/840333/files/crc_baxter_results.tar.gz"
tar -xzvf crc_baxter_results.tar.gz
cd ..
```

Next, we'll read in the unzipped OTU table and metadata files into R.

We'll need to manually define which disease labels are of interest and what metadata
column contains the sample IDs (i.e. the IDs in the OTU table).

```{r, baxter-readdata}
otu_path = paste0(datdir, "crc_baxter_results/RDP/crc_baxter.otu_table.100.denovo.rdp_assigned")
meta_path = paste0(datdir, "crc_baxter_results/crc_baxter.metadata.txt")

# DiseaseState labels to keep
labels <- c("H", "CRC")

# Metadata column that has sample IDs
col_label <- "Sample_Name_s"

# load OTU table and metadata
otu <- read.table(otu_path)
meta <- read.csv(meta_path, sep='\t')

# Keep only samples with the right DiseaseState metadata
meta <- meta %>% filter(DiseaseState %in% labels)

# Add "X" in front of sample IDs because of how R read in the OTU table
meta[, 1] <- sub("^", "X", meta[, 1])

# Keep only samples with both metadata and 16S data
keep_samples <- intersect(colnames(otu), meta[, 1])
otu <- otu[, keep_samples]
meta <- meta %>% filter(get(col_label) %in% keep_samples)

```

A brief aside: what's the count distribution of these OTUs?

```{r, eval=FALSE}
otu %>% colSums %>% hist()
```

Since we'll be using OTU-wise covariates, we shouldn't need to perform any
filtering/cleaning of the OTUs, apart from removing any that are all zeros. 
(This may happen after removing shallow samples, I think.)
However, let's make sure we get rid of any samples with too few reads. 
We define the minimum number of reads per sample in `sample_reads`. 

After we've removed any shallow samples, we'll convert the OTU table to relative
abundances. I'll add a pseudo-count of 1e-7 to any zero entries, to avoid
problems with taking logs.

```{r, baxter-cleandata}
remove_shallow_smpls <- function(df, n_reads) {
  # Removes samples with fewer than n_reads from dataframe df.
  # df has OTUs in rows and samples in columns
  return(df[, colSums(df) >= n_reads])
  }

remove_shallow_otus <- function(df, n_reads){
  return(df[rowSums(df) > n_reads, ])
  }

remove_rare_otus <- function(df, perc_samples){
  return(df[rowSums(df > 0) / dim(df)[2] > perc_samples, ])
}

## Remove OTUs with fewer than 10 reads
otu <- remove_shallow_otus(otu, 10)
print(dim(otu))
## Remove OTUs in fewer than 1% of samples
otu <- remove_rare_otus(otu, 0.01)
print(dim(otu))

# Remove samples with fewer than sample_Reads reads
sample_reads <- 100
otu <- remove_shallow_smpls(otu, sample_reads)
print(dim(otu))

# Update metadata with new samples
meta <- meta %>% filter(get(col_label) %in% colnames(otu))

# Remove empty OTUs
otu <- otu[rowSums(otu) > 0, ]
print(dim(otu))

# Convert to relative abundance
abun_otu <- t(t(otu) / rowSums(t(otu)))

# Add pseudo counts
#minabun <- min(abun_otu[abun_otu > 0]) # Use this to figure out what pseudo-count value to add
zeroabun <- 0
abun_otu <- abun_otu + zeroabun

```

Next, we need to calculate the pvalues, effect size, and standard error for each OTU.
Here, we'll compare CRC vs. healthy. We won't consider the nonCRC adenoma patients.
We'll put these results into a dataframe, and label the columns with the 
standardized names for downstream use (`pval`, `SE`, `effect_size`, `test_statistic`).
The test statistic is the one returned by `wilcox.test()`.

Note that the effect here is calculated as logfold change of mean abundance in controls
relative to cases (i.e. `log(mean_abun[controls]/mean_abun[cases])`)

While we're at it, we'll also calculate the mean abundance and ubiquity (detection rate)
of each OTU. Later, we can assign their values to a new column called `ind_covariate` 
for use in downstream steps.

```{r, baxter-pvals}
resfile <- paste0(resdir, "baxter_results_",
                  nrow(abun_otu), "_OTUs.RData")
if (!file.exists(resfile)){
  # Get case and control indices
  case_idx <- meta$DiseaseState %in% c("CRC")
  ctrl_idx <- meta$DiseaseState %in% c("H")
  
  # Calculate pvalues, effects, and stderr
  pvals <- c()
  teststats <- c()
  ses <- c()
  effs <- c()
  mean_abuns <- c()
  mean_abuns_present <- c()
  ubis <- c()
  
  casedf <- abun_otu[, case_idx]
  ctrldf <- abun_otu[, ctrl_idx]
  for (o in rownames(abun_otu)) {
    # wilcoxon p value  
    w <- wilcox.test(casedf[o, ], 
                     ctrldf[o, ])
    p <- w$p.value
    teststat <- w$statistic
    
    pvals <- c(pvals, p)
    teststats <- c(teststats, teststat)
    
    # standard error of the OTU abundance, across all samples
    ses <- c(ses, 
             sd(abun_otu[o, ])/sqrt(length(abun_otu[o, ]))
             )
    
    # mean OTU abundance across all samples (after removing pseudo-count)
    mean_abuns <- c(mean_abuns, 
                    mean(abun_otu[o, ] - zeroabun)
                    )
    
    # mean OTU abundance across only samples with the OTU present
    mean_abuns_present <- c(mean_abuns_present, 
                            sum(abun_otu[o, ] - zeroabun) / sum(abun_otu[o, ] > zeroabun)
                            )
    
    # ubiquity of OTU across all samples
    ubis <- c(ubis, 
              sum(abun_otu[o, ] > zeroabun) / length(abun_otu[o, ])
              )
    
    # effect (logfold difference)
    effs <- c(effs, 
              log(mean(abun_otu[o, ctrl_idx])/mean(abun_otu[o, case_idx]))
              )
  
  }
  
  res <- data.frame(otu = rownames(abun_otu), 
                    pval = pvals, test_statistic = teststats,
                    SE = ses, effect_size = effs, 
                    mean_abun = mean_abuns, mean_abun_present = mean_abuns_present,
                    ubiquity = ubis)
  res <- res %>%
    mutate(test_statistic = qnorm(exp(log(pval)-log(2)), lower.tail=FALSE) * sign(effect_size))
  
  save(res, file=resfile)
  
}else{
  load(resfile)
}
head(res)
```

Finally, let's try to add phylogeny as covariates. Here we'll have columns for each separate taxonomic level.

```{r, baxter-addphylo}
res <- res %>% separate(otu, 
                        c("kingdom", "phylum", "class", "order", "family", "genus", "species", "denovo"), 
                        sep=";", remove = FALSE)

```

#### Check Covariate Diagnostics

Here we look to see if the covariates do indeed look informative.

##### Ubiquity

```{r, baxter-ubi, fig.width=10, fig.height=3.5}
strat_hist(res, pvalue="pval", covariate="ubiquity", maxy=10, binwidth=0.05)
rank_scatter(res, pvalue="pval", covariate="ubiquity")
```

Something weird is happening with p=0.20 and p=0.40 - maybe this has something to do with ties?
Either way, ubiquity looks to be a bit informative (from the scatter plot, but not really the histograms...)

##### Mean abundance (across non-zero samples)

```{r, baxter-abun, fig.width=10, fig.height=3.5}
strat_hist(res, pvalue="pval", covariate="mean_abun_present", maxy=8, binwidth=0.05)
rank_scatter(res, pvalue="pval", covariate="mean_abun_present")
```

##### Phylogeny

Let's look at phylum-level stratification first. A priori, I *might* expect
Proteobacteria to be enriched for low p-values? But I don't know if that's
super legit, and Eric doesn't seem to think that phylogeny will be informative at all...

```{r, baxter-phylo, fig.width=10, fig.height=6, eval=FALSE}
#strat_hist(res, pvalue="pval", covariate="mean_abun_present", maxy=20)
#rank_scatter(res, pvalue="pval", covariate="ubiquity")
ggplot(res, aes(x=pval)) + geom_histogram() + facet_wrap(~phylum, scales = "free")
```

Not really informative either...

Let's use `ubiquity` as our `ind_covariate`

```{r, baxter-labelcovariate}
res <- res %>% mutate(ind_covariate = ubiquity)
```

Also (hacky), remove test_statistic so that ash doesn't run.

```{r, baxter-rm-ash}
res <- res %>% 
  mutate(test_statistic = NA) %>%
  mutate(effect_size = NA)
```

#### Set up BenchDesign object 

First, we'll create an object of `BenchDesign` class to hold the data and 
add the benchmark methods to the `BenchDesign` object. We'll do this for a few
values of nmids, to explore its effects on the outputs.

Then, we'll construct the `SummarizedBenchmark` object, which will run
the functions specified in each method (these are actually sourced in from the
helper scripts). 

```{r, baxter-sb, results="hide", message=FALSE, eval=TRUE}
for (nmids in c(5, 50, 150)){
  print(nmids)
  bd <- initializeBenchDesign(nmids=nmids)
  
  resfile <- paste0(resdir, "baxter_summarizedBenchmark_",
                    nrow(res), "_nmids_", nmids, ".RData")
  duration <- NA
  if (!file.exists(resfile)){
    t1 <- proc.time()
    sb <- bd %>% buildBench(data=res, ftCols = "ind_covariate")
                            #parallel=TRUE, BPPARAM=multicoreParam)
    metadata(sb)$data_download_link <- "https://zenodo.org/record/840333/files/crc_baxter_results.tar.gz"
    save(sb, file=resfile)
    duration <- round((proc.time()-t1)[3]/60,1)
  }else{
    load(resfile)
  }
}

```

Just for future runs, where I load the file instead of re-calculating it, here are the errors I got:

```
[1] 5
Squarem-1
Objective fn: 944053
Objective fn: 111059  Extrapolation: 0  Steplength: 1
Objective fn: 92567.2  Extrapolation: 1  Steplength: 2.85015
Objective fn: 91136.8  Extrapolation: 1  Steplength: 2.70993
Objective fn: 90930.1  Extrapolation: 1  Steplength: 4
.
.
.
Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !NaNs produced<simpleError in if (sum(D^2) > qchisq(0.9, nmids - 2 - df)) {    warning(paste0("f(z) misfit = ", round(D, 1), ".  Rerun with increased df."))}: missing value where TRUE/FALSE needed>

[1] 50
Squarem-1
Objective fn: 944053
Objective fn: 111059  Extrapolation: 0  Steplength: 1
Objective fn: 92567.2  Extrapolation: 1  Steplength: 2.85015
Objective fn: 91136.8  Extrapolation: 1  Steplength: 2.70993
Objective fn: 90930.1  Extrapolation: 1  Steplength: 4
.
.
.
Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !f(z) misfit = -33.4.  Rerun with increased df.f(z) misfit = -40.5.  Rerun with increased df.f(z) misfit = 207.5.  Rerun with increased df.f(z) misfit = -33.  Rerun with increased df.f(z) misfit = -13.  Rerun with increased df.f(z) misfit = -35.  Rerun with increased df.f(z) misfit = -44.7.  Rerun with increased df.f(z) misfit = -38.4.  Rerun with increased df.f(z) misfit = -35.2.  Rerun with increased df.f(z) misfit = -59.5.  Rerun with increased df.f(z) misfit = 275.  Rerun with increased df.f(z) misfit = -30.1.  Rerun with increased df.f(z) misfit = -74.7.  Rerun with increased df.f(z) misfit = -87.9.  Rerun with increased df.f(z) misfit = -85.1.  Rerun with increased df.f(z) misfit = 268.2.  Rerun with increased df.f(z) misfit = -56.9.  Rerun with increased df.f(z) misfit = -39.4.  Rerun with increased df.f(z) misfit = -8.8.  Rerun with increased df.f(z) misfit = -35.6.  Rerun with increased df.f(z) misfit = -30.2.  Rerun with increased df.f(z) misfit = -14.3.  Rerun with increased ... <truncated>

[1] 150
Squarem-1
Objective fn: 944053
Objective fn: 111059  Extrapolation: 0  Steplength: 1
Objective fn: 92567.2  Extrapolation: 1  Steplength: 2.85015
Objective fn: 91136.8  Extrapolation: 1  Steplength: 2.70993
Objective fn: 90930.1  Extrapolation: 1  Steplength: 4
Objective fn: 90882.9  Extrapolation: 0  Steplength: 1
.
.
.
Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !Censored sample for null model estimation has only size 0 !f(z) misfit = -15.9.  Rerun with increased df.f(z) misfit = 8.3.  Rerun with increased df.f(z) misfit = -10.8.  Rerun with increased df.f(z) misfit = -18.1.  Rerun with increased df.f(z) misfit = -24.6.  Rerun with increased df.f(z) misfit = -27.1.  Rerun with increased df.f(z) misfit = -24.6.  Rerun with increased df.f(z) misfit = -18.8.  Rerun with increased df.f(z) misfit = -21.4.  Rerun with increased df.f(z) misfit = 279.5.  Rerun with increased df.f(z) misfit = 83.1.  Rerun with increased df.f(z) misfit = -27.4.  Rerun with increased df.f(z) misfit = -28.3.  Rerun with increased df.f(z) misfit = -7.4.  Rerun with increased df.f(z) misfit = 11.6.  Rerun with increased df.f(z) misfit = -18.2.  Rerun with increased df.f(z) misfit = -23.  Rerun with increased df.f(z) misfit = -16.7.  Rerun with increased df.f(z) misfit = -24.1.  Rerun with increased df.f(z) misfit = -23.2.  Rerun with increased df.f(z) misfit = -24.4.  Rerun with increased df.f(z) misfit = -26.6.  Rerun with increase... <truncated>> 
```

Next, we'll add the default performance metric for q-value assays. First, we have
to rename the assay to 'qvalue'.

```{r, baxter-metrics, eval=TRUE}
# rename assay to qvalue
assayNames(sb) <- "qvalue"
sb <- addDefaultMetrics(sb)
```

Now, we'll plot the results.

Debugging note: to look at the matrix of qvalues, run `assays(sb)[["qvalue"]]`

```{r, baxter-plot, width=15, height=15, eval=TRUE}
# plot nrejects by method overall and stratified by covariate
rejections_scatter( sb, as_fraction=FALSE, supplementary=FALSE)
rejection_scatter_bins(sb, covariate="ind_covariate", supplementary=FALSE)
#plotFDRMethodsOverlap(sb, alpha=0.1, supplementary=FALSE, order.by="freq", nsets=100 )
```

```{r, eval=FALSE}
assays(sb)[["qvalue"]][, "ashs"] %>% max()
sum(is.na(assays(sb)[["qvalue"]][, "scott-empirical"]))
sum(is.na(assays(sb)[["qvalue"]][, "lfdr"]))

```

## Baxter, genus-level analysis

```{r, baxter-genus}
## Collapse baxter OTU table to genus level

# Add column with otu names
genus_df <- as.data.frame(abun_otu)
genus_df$otu <- rownames(genus_df)
# Melt into tidy format
genus_df <- genus_df %>% melt(id.var="otu", variable.name="sample", value.name="abun")
# Split into separate taxonomic levels
genus_df <- genus_df %>% 
  separate(otu, c("kingdom", "phylum", "class", "order", "family", "genus", "species", "denovo"), 
           sep=";", remove = FALSE)

# Get rid of unannoated genera, and sum abundances for genera
genus_df <- genus_df %>% 
  filter(genus != "g__") %>% 
  group_by(genus, sample) %>% 
  summarise(total_abun = sum(abun)) 

# Convert back to longform
genus_df <- genus_df %>% spread(genus, total_abun) %>% as.data.frame
rownames(genus_df) <- genus_df$sample
genus_df <- genus_df %>% select(-sample) %>% t

# And re-order columns to match metadata
genus_df <- genus_df[, match(meta$Sample_Name_s, colnames(genus_df))]
```

```{r, baxter-genus-pvals}
resfile <- paste0(resdir, "baxter_results_",
                  nrow(genus_df), "_genera.RData")
if (!file.exists(resfile)){
  # Get case and control indices
  case_idx <- meta$DiseaseState %in% c("CRC")
  ctrl_idx <- meta$DiseaseState %in% c("H")
  
  # Calculate pvalues, effects, and stderr
  pvals <- c()
  teststats <- c()
  ses <- c()
  effs <- c()
  mean_abuns <- c()
  mean_abuns_present <- c()
  ubis <- c()
  
  casedf <- genus_df[, case_idx]
  ctrldf <- genus_df[, ctrl_idx]
  for (o in rownames(genus_df)) {
    # wilcoxon p value  
    w <- wilcox.test(casedf[o, ], 
                     ctrldf[o, ])
    #w <- kruskal.test(list(casedf[o, ], ctrldf[o, ]))
    p <- w$p.value
    teststat <- w$statistic
    
    pvals <- c(pvals, p)
    teststats <- c(teststats, teststat)
    
    # standard error of the OTU abundance, across all samples
    ses <- c(ses, 
             sd(genus_df[o, ])/sqrt(length(genus_df[o, ]))
             )
    
    # mean OTU abundance across all samples (after removing pseudo-count)
    mean_abuns <- c(mean_abuns, 
                    mean(genus_df[o, ] - zeroabun)
                    )
    
    # mean OTU abundance across only samples with the OTU present
    mean_abuns_present <- c(mean_abuns_present, 
                            sum(genus_df[o, ] - zeroabun) / sum(genus_df[o, ] > zeroabun)
                            )
    
    # ubiquity of OTU across all samples
    ubis <- c(ubis, 
              sum(genus_df[o, ] > zeroabun) / length(genus_df[o, ])
              )
    
    # effect (logfold difference)
    effs <- c(effs, 
              log(mean(genus_df[o, ctrl_idx])/mean(genus_df[o, case_idx]))
              )
  
  }
  
  res <- data.frame(otu = rownames(genus_df), 
                    pval = pvals, test_statistic = teststats,
                    SE = ses, effect_size = effs, 
                    mean_abun = mean_abuns, mean_abun_present = mean_abuns_present,
                    ubiquity = ubis)
  res <- res %>%
    mutate(test_statistic = qnorm(exp(log(pval)-log(2)), lower.tail=FALSE) * sign(effect_size))
  
  save(res, file=resfile)
  
}else{
  load(resfile)
}
```

#### Check Covariate Diagnostics

Here we look to see if the covariates do indeed look informative.

##### Ubiquity

```{r, baxter-genus-ubi, fig.width=10, fig.height=3.5}
strat_hist(res, pvalue="pval", covariate="ubiquity", maxy=10, binwidth=0.05)
rank_scatter(res, pvalue="pval", covariate="ubiquity")
```

Something weird is happening with p=0.20 and p=0.40 - maybe this has something to do with ties?
Either way, ubiquity looks to be a bit informative (from the scatter plot, but not really the histograms...)

##### Mean abundance (across non-zero samples)

```{r, baxter-genus-abun, fig.width=10, fig.height=3.5}
strat_hist(res, pvalue="pval", covariate="mean_abun_present", maxy=8, binwidth=0.05)
rank_scatter(res, pvalue="pval", covariate="mean_abun_present")
```

Let's use `ubiquity` as our `ind_covariate`

```{r, baxter-genus-labelcovariate}
res <- res %>% mutate(ind_covariate = ubiquity)
```

Also (hacky), remove test_statistic so that ash doesn't run.

```{r, baxter-genus-rm-ash}
res <- res %>% 
  mutate(test_statistic = NA) %>%
  mutate(effect_size = NA)
```

#### Set up BenchDesign object 

First, we'll create an object of `BenchDesign` class to hold the data and 
add the benchmark methods to the `BenchDesign` object. We'll do this for a few
values of nmids, to explore its effects on the outputs.

Then, we'll construct the `SummarizedBenchmark` object, which will run
the functions specified in each method (these are actually sourced in from the
helper scripts). 

```{r, baxter-genus-sb, results="hide", message=FALSE, eval=TRUE}
for (nmids in c(5, 50, 150)){
  print(nmids)
  bd <- initializeBenchDesign(nmids=nmids)
  
  resfile <- paste0(resdir, "baxter_summarizedBenchmark_",
                    nrow(res), "_genera_nmids_", nmids, ".RData")
  duration <- NA
  if (!file.exists(resfile)){
    t1 <- proc.time()
    sb <- bd %>% buildBench(data=res, ftCols = "ind_covariate")
                            #parallel=TRUE, BPPARAM=multicoreParam)
    metadata(sb)$data_download_link <- "https://zenodo.org/record/840333/files/crc_baxter_results.tar.gz"
    save(sb, file=resfile)
    duration <- round((proc.time()-t1)[3]/60,1)
  }else{
    load(resfile)
  }
}

```


Next, we'll add the default performance metric for q-value assays. First, we have
to rename the assay to 'qvalue'.

```{r, baxter-genus-metrics, eval=TRUE}
# rename assay to qvalue
assayNames(sb) <- "qvalue"
sb <- addDefaultMetrics(sb)
```

Now, we'll plot the results.

Debugging note: to look at the matrix of qvalues, run `assays(sb)[["qvalue"]]`

```{r, baxter-genus-plot, width=15, height=15, eval=TRUE}
# plot nrejects by method overall and stratified by covariate
rejections_scatter( sb, as_fraction=FALSE, supplementary=FALSE)
rejection_scatter_bins(sb, covariate="ind_covariate", supplementary=FALSE)
#plotFDRMethodsOverlap(sb, alpha=0.1, supplementary=FALSE, order.by="freq", nsets=100 )
```

Note: Benjamini-Hochberg (bh) overlaps exactly with the IHW results.

```{r, baxter-genus-overlap}
plotFDRMethodsOverlap(sb, alpha=0.1, supplementary=FALSE, order.by="freq", nsets=100 )
```
```{r}
methods <- c( "lfdr", "ihw-a10", "bl-df03", "qvalue", "bh", "bonf" )
plotCovariateBoxplots( sb, alpha=0.1, nsets=6, methods=methods)
```