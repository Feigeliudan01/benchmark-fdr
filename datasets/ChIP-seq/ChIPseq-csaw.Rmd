---
title: "Case Study: ChIP-seq Differential Peak Calling (csaw)"
author: "Patrick Kimes"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: 
   html_document:
        toc: true
        toc_float: true
        highlight: tango
        number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Summary

In this case study, we perform differential peak calling on ChIP-seq data of the histone
H3K4Me3 for samples from two cell lines (K562 and Gm12878) using publicly available data
generated by the ENCODE Project. The same data set is used for all ChIP-seq differential
testing case studies. 

The _csaw_ package provides a powerful and flexible approach to differential peak calling
based on counting reads in sliding windows across the genome. Unlike the _DiffBind_ approach
which restricts testing to peaks called from the same samples, the sliding window approach
is able to detect differential peaks over sharper or wider regions, without being constrained
by the original peak boundaries obtained from the peak calling algorithm, e.g. _MACS_.
Differential peaks are called at each window using _edgeR_ and p-values are combined within
adjacent windows (clusters).

For this analysis, the default _csaw_ settings are used as described in the _csaw_ "User Guide."

# Workspace Setup

```{r, wkspace-setup, results='hide', message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(SummarizedBenchmark)
library(BiocParallel)
library(rtracklayer)
library(Rsamtools)
library(Rsubread)
library(csaw)
library(edgeR)

## load helper functions
for (f in list.files("../R", "\\.(r|R)$", full.names = TRUE)) {
    source(f)
}

## project data/results folders
datdir <- "data"
resdir <- "results"
dir.create(datdir, showWarnings = FALSE)
dir.create(resdir, showWarnings = FALSE)

## intermediary files we create below
count_file <- file.path(resdir, "h3k4me3-csaw-counts.rds")
result_file <- file.path(resdir, "h3k4me3-csaw-results.rds")
bench_file <- file.path(resdir, "h3k4me3-csaw-benchmark.rds")

## set up parallel backend
cores <- as.numeric(Sys.getenv("SLURM_NTASKS"))
multicoreParam <- MulticoreParam(workers = cores)
```

# Data Preparation

We download the fastq files directly from the UCSC ENCODE portal and align them against the reference.

```{r, download-fastqs}
broad_url <- "http://hgdownload.soe.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeBroadHistone/"
broad_fastqs <- c("wgEncodeBroadHistoneGm12878H3k4me3StdRawDataRep1.fastq.gz",
                  "wgEncodeBroadHistoneGm12878H3k04me3StdRawDataRep2V2.fastq.gz",
                  "wgEncodeBroadHistoneK562H3k4me3StdRawDataRep1.fastq.gz",
                  "wgEncodeBroadHistoneK562H3k4me3StdRawDataRep2.fastq.gz")

for (i_fq in broad_fastqs) {
    if (!file.exists(file.path(datdir, i_fq))) {
        download.file(paste0(broad_url, i_fq),
                      destfile = file.path(datdir, i_fq))
    }
}

uw_url <- "http://hgdownload.soe.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeUwHistone/"
uw_fastqs <- c("wgEncodeUwHistoneGm12878H3k4me3StdRawDataRep1.fastq.gz",
               "wgEncodeUwHistoneGm12878H3k4me3StdRawDataRep2.fastq.gz",
               "wgEncodeUwHistoneK562H3k4me3StdRawDataRep1.fastq.gz",
               "wgEncodeUwHistoneK562H3k4me3StdRawDataRep2.fastq.gz")

for (i_fq in uw_fastqs) {
    if (!file.exists(file.path(datdir, i_fq))) {
        download.file(paste0(uw_url, i_fq),
                      destfile = file.path(datdir, i_fq))
    }
}
```

We also use the GRCh38 reference genome used for ENCODE3 analysis. If the reference index is
not already available, we must first download the reference file. Note that this is a different
reference than what was originally used for the data.

```{r, download-ref}
ref_url <- paste0("https://www.encodeproject.org/files/",
                  "GRCh38_no_alt_analysis_set_GCA_000001405.15/@@download/",
                  "GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta.gz")
ref_fastagz <- file.path(datdir, basename(ref_url))
ref_fasta <- gsub("\\.gz$", "", ref_fastagz)

if (!file.exists(file.path(datdir, "ref_index.00.b.tab"))) {
    if (!file.exists(ref_fasta)) {
        download.file(ref_url, destfile = ref_fastagz)
        system(paste("gunzip", ref_fastagz))
    }
    buildindex(basename = file.path(datdir, "ref_index"),
               reference = ref_fasta)
}
```

We determine sample metadata from the file names.

```{r}
fqfiles <- file.path(datdir, c(broad_fastqs, uw_fastqs))
labs <- gsub("wgEncode(.*)Histone.*", "\\1", basename(fqfiles))
cells <- gsub("wgEncode.*Histone(.*)H3k.*", "\\1", basename(fqfiles))

meta <- data.frame(cellline = cells, lab = labs,
                   fqfile = fqfiles,
                   bamfile = gsub("\\.fastq.gz", ".bam", fqfiles),
                   sortedfile = gsub("\\.fastq.gz", ".sorted.bam", fqfiles),
                   stringsAsFactors = FALSE)
```

We also download blacklisted regions for GRCh38 from ENCODE (https://www.encodeproject.org/annotations/ENCSR636HFF/).

```{r}
blacklist_url <- paste0("http://mitra.stanford.edu/kundaje/akundaje/",
                        "release/blacklists/hg38-human/hg38.blacklist.bed.gz")
if (!file.exists(file.path(datdir, basename(blacklist_url)))) {
    download.file(blacklist_url, destfile = file.path(datdir, basename(blacklist_url)))
}

bl <- import(file.path(datdir, basename(blacklist_url)))
```

Standard set of parameters are defined for the analysis. Only the canonical set of chromosomes are used in the analysis.

```{r}
std_chr <- paste0("chr", c(1:19, "X", "Y"))
param <- readParam(minq = 50, discard = bl, restrict = std_chr)
```

## Read Counting 

We count reads within sliding windows across the genome.

```{r count, results = 'hide', message = FALSE, warning = FALSE}
if (file.exists(count_file)) {
    filtered_cnts <- readRDS(count_file)
} else {
    ## align reads to reference
    unaligned <- !file.exists(meta$bamfile)
    if (any(unaligned)) {
        align(index = file.path(datdir, "ref_index"),
              readfile1 = meta$fqfile[unaligned],
              TH1 = 2, type = 1,
              output_file = meta$bamfile[unaligned])
    }
    
    ## sort bam files
    for (i in 1:nrow(meta)) {
        if (!file.exists(meta$sortedfile[i])) {
            sortBam(meta$bamfile[i], gsub("\\.bam$", "", meta$sortedfile[i]))
        }
    }
    
    ## mark duplicates w/ picard
    temp_bam <- "temp_dups.bam"
    temp_file <- "temp_mets.txt"
    temp_dir <- "data/temp_dups"
    dir.create(temp_dir)
    for (i_bam in meta$sortedfile) {
        code <- paste0("java -jar ${PICARD_TOOLS_HOME}/picard.jar ",
                       "MarkDuplicates I=%s O=%s M=%s ", 
                       "TMP_DIR=%s AS=true REMOVE_DUPLICATES=false ",
                       "VALIDATION_STRINGENCY=SILENT")
        code <- sprintf(code, i_bam, temp_bam, temp_file, temp_dir)
        code <- system(code)
        stopifnot(code == 0L)
        file.rename(temp_bam, i_bam)
    }
    unlink(temp_file)
    unlink(temp_dir, recursive = TRUE)
    
    ## index bam files
    indexBam(meta$sortedfile)

    ## use correlateReads to determine fragment length (remove dups)
    x <- correlateReads(meta$sortedfile, param = reform(param, dedup = TRUE))
    frag_len <- which.max(x) - 1

    ## count reads in sliding windows (keep dups)
    win_cnts <- windowCounts(meta$sortedfile, param = param,
                             width = 150, ext = frag_len)

    ## filter windows by abundance
    bins <- windowCounts(meta$sortedfile, bin = TRUE, width = 2000, param = param)
    filter_stat <- filterWindows(win_cnts, bins, type = "global")
    keep <- filter_stat$filter > log2(3)
    filtered_cnts <- win_cnts[keep, ]
    
    saveRDS(filtered_cnts, file = count_file)
}
```

# Data Analysis

## Differential Testing

We use _edgeR_ to test for differential binding.

```{r test, results='hide', message=FALSE, warning=FALSE}
if (file.exists(result_file)) {
    res_ranges <- readRDS(result_file)
} else {
    
    ## normalize for library-specific trended biases
    offsets <- normOffsets(filtered_cnts, type = "loess")

    ## create model design
    design <- model.matrix(~ lab + cellline,
                           data = as.data.frame(meta))
    
    ## run quasi-likelihood F-test
    y <- asDGEList(filtered_cnts)
    y$offset <- offsets
    y <- estimateDisp(y, design)
    fit <- glmQLFit(y, design, robust = TRUE)
    res <- glmQLFTest(fit)
    
    ## merge p-values across regions
    merged <- mergeWindows(rowRanges(filtered_cnts), tol = 100, max.width = 5000)
    tab_comb <- combineTests(merged$id, res$table)
    tab_best <- getBestTest(merged$id, res$table)
    
    ## save results
    res_ranges <- merged$region
    elementMetadata(res_ranges) <-
        data.frame(tab_comb,
                   best_pos = mid(ranges(rowRanges(filtered_cnts[tab_best$best]))),
                   best_logFC = tab_best$logFC)
    
    saveRDS(res_ranges, file = result_file)
}
```

# Session Info

```{r}
sessionInfo()
```
